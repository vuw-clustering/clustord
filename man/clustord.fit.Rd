% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clustering.R
\name{clustord.fit}
\alias{clustord.fit}
\title{Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional
Odds Models (POM) or Binary Models}
\usage{
clustord.fit(
  formula,
  model,
  nclus.row = NULL,
  nclus.column = NULL,
  long.df,
  initvect = NULL,
  pi.init = NULL,
  kappa.init = NULL,
  EM.control = default.EM.control(),
  optim.method = "L-BFGS-B",
  optim.control = default.optim.control(),
  constraint_sum_zero = TRUE,
  start_from_simple_model = TRUE,
  nstarts = 5
)
}
\arguments{
\item{optim.control}{control list for the \code{optim} call within the M step
of the EM algorithm. See the control list Details in the \code{optim}
manual for more info.}

\item{constraint_sum_zero}{(default \code{TRUE}) if \code{TRUE}, use constraints
that cluster effects sum to zero; if \code{FALSE}, use constraints that
the cluster effect for the first cluster will be 0.
Both versions have the same constraints for joint row-column cluster
effects: these effects are described by a matrix of parameters gamma_rc,
indexed by row cluster and column cluster indices, and the constraints
are that the final column of gamma_rc is equal to the negative sum of the
other columns (so \code{gamma} columns sum to zero) and first row of
gamma_rc is equal to the negative sum of the other rows (so \code{gamma}
rows sum to zero).}

\item{formula:}{model formula (see 'Details').}

\item{model:}{\code{"OSM"} for Ordered Stereotype Model or \code{"POM"} for
Proportional Odds Model or \code{"Binary"} for binary data model.}

\item{nclus.row:}{number of row clustering groups.}

\item{nclus.column:}{number of column clustering groups.}

\item{long.df:}{data frame with at least three columns, \code{Y} and \code{ROW}
and \code{COL}. Each row in the data frame corresponds to a single cell
in the original data matrix; the response value in that cell is given by
\code{Y}, and the row and column indices of that cell in the matrix are
given by \code{ROW} and \code{COL}. Use \code{\link{mat2df}} to create
this data frame from your data matrix of responses.
\code{\link{mat2df}} also allows you to supply data frames of row or
column covariates which will be incorporated into \code{long.df}.}

\item{initvect:}{(default NULL) vector of starting parameter values for the model.
    Note: if the user enters an initial vector of parameter values, it is
    \strong{strongly recommend} that the user also check the values of
    \code{parlist.init} in the output object, to \strong{make sure that the
    constructed parameters are as expected}.

    If \code{NULL}, starting parameter values will be generated automatically.

    See 'Details' for definitions of the parameters used for different models.}

\item{pi.init:}{(default \code{NULL}) starting parameter values for the proportions
    of observations in the different row clusters.

    If \code{NULL}, starting values will be generated automatically.

    User-specified values of \code{pi.init} must be of length \code{(nclus.row-1)}
    because the final value will be automatically calculated so that the
    values of \code{pi} sum to 1.}

\item{kappa.init:}{(default \code{NULL}) starting parameter values for the
    proportions of observations in the different column clusters.

    If \code{NULL}, starting values will be generated automatically.

    User-specified values of \code{kappa.init} must be of length
    \code{(nclus.column-1)} because the final value will be automatically
    calculated so that the values of \code{kappa} sum to 1.}

\item{EM.control:}{(default = \code{list(EMcycles=50, EMlikelihoodtol=1e-4,
    EMparamtol=1e-2, paramstopping=TRUE, startEMcycles=10, keepallparams=FALSE,
    epsilon=1e-6)})
    list of parameters controlling the EM algorithm.

    \code{EMcycles} controls how many EM iterations of the main EM algorithm
    are used to fit the chosen submodel.

    \code{EMlikelihoodtol} is the tolerance for the stopping criterion for
    the \strong{log-likelihood} in the EM algorithm. The criterion is the
    absolute change in the \strong{incomplete} log-likelihood since the
    previous iteration, scaled by the size of the dataset \code{n*p}, where
    \code{n} is the number of rows in the data matrix and \code{p} is the
    number of columns in the data matrix. The scaling is applied because the
    incomplete log-likelihood is predominantly affected by the dataset size.

    \code{EMparamtol} is the tolerance for the stopping criterion for the
    \strong{parameters} in the EM algorithm. This is a tolerance for the
    \strong{sum} of the scaled parameter changes from the last iteration,
    i.e. the tolerance is not for any individual parameter but for the sum of
    changes in all the parameters. Thus the default tolerance is 1e-2.
    The individual parameter criteria are the absolute differences between
    the exponentiated absolute parameter value at the current timestep and
    the exponentiated absolute parameter value at the previous timestep, as a
    proportion of the exponentiated absolute parameter value at the current
    timestep. The exponentiation is to rescale parameter values that are
    close to zero.

    there are around 5 independent parameter values, then at the point of
    convergence using default tolerances for the log-likelihood and the
    parameters, each parameter will have a scaled absolute change since the
    previous iteration of about 1e-4; if there are 20 or 30 independent
    parameters, then each will have a scaled aboslute change of about 1e-6.

    \code{paramstopping}: if \code{FALSE}, indicates that the EM algorithm
    should only check convergence based on the change in incomplete-data
    log-likelihood, relative to the current difference between the
    complete-data and incomplete-data log-likelihoods, i.e.
    \code{abs(delta_lli)/abs(llc[iter] - lli[iter])};
    if \code{TRUE}, indicates that as well as checking the likelihood
    criterion, the EM algorithm should also check whether the relative change
    in the exponentials of the absolute values of the current parameters is
    below the tolerance \code{EMstoppingpar}, to see whether the parameters
    and the likelihood have both converged.

    \code{startEMcycles} controls how many EM iterations are used when
    fitting the simpler submodels to get starting values for fitting models
    with interaction.

    \code{keepallparams}: if true, keep a record of parameter values
    (including pi_r and kappa_c) for every EM iteration.

    For \code{columnclustering}, the parameters saved from each iteration
    will NOT be converted to column clustering format, and will be in the row
    clustering format, so \code{alpha} in
    \code{EM.status$params.every.iteration} will correspond to beta_c and
    \code{pi} will correspond to kappa.

    \code{epsilon}: default 1e-6, small value used to adjust values of pi,
    kappa and theta that are too close to zero so that taking logs of them
    does not create infinite values.}

\item{optim.method:}{(default "L-BFGS-B") method to use in optim within the M
step of the EM algorithm. Must be one of 'L-BFGS-B', 'BFGS', 'CG' or
'Nelder-Mead' (i.e. not the SANN method).}

\item{start_from_simple_model:}{(default \code{TRUE}) if \code{TRUE}, fit a
simpler clustering model first and use that to provide starting values for
all parameters for the model with interactions;
if \code{FALSE}, use the more basic models to provide starting values only
for \code{pi.init} and \code{kappa.init}.
If the full model has interaction terms, then simpler models are ones
without the interactions. If the model has individual row/column effects
alongside the clusters, then simpler models are ones without the individual
row/column effects. If the full model has covariates, then simpler models
are ones without the covariates (to get starting values for the cluster
parameters), and ones with the covariates but no clustering (to get
starting values for the covariates).}
}
\value{
A list with components:

    \code{info}: Basic info n, p, q, the number of parameters, the number of
    row clusters and the number of column clusters, where relevant.

    \code{model}: The model used for fitting, "OSM" for Ordered Stereotype
    Model, "POM" for Proportional Odds Model, or "Binary" for Binary model.

    \code{EM.status}: a list containing the latest iteration \code{iter},
    latest incomplete-data and complete-data log-likelihoods \code{new.lli}
    and \code{new.llc}, the best incomplete-data log-likelihood \code{best.lli}
    and the corresponding complete-data log-likelihood, \code{llc.for.best.lli},
    and the parameters for the best incomplete-data log-likelihood,
    \code{params.for.best.lli}, indicator of whether the algorithm converged
    \code{converged}, and if the user chose to keep all parameter values from
    every iteration, also \code{params.every.iteration}.

    Note that for \strong{biclustering}, i.e. when \code{ROWCLUST} and
    \code{COLCLUST} are both included in the model, the \strong{incomplete}
    log-likelihood is calculated using the entropy approximation, and this
    may be \strong{inaccurate} unless the algorithm has converged or is close
    to converging. So beware of using the incomplete log-likelihood and the
    corresponding AIC value \strong{unless the EM algorithm has converged}.

    \code{criteria}: the calculated values of AIC, BIC,
    etc. from the best incomplete-data log-likelihood.

    \code{epsilon}: the very small value (default 1e-6) used to adjust values
    of pi and kappa and theta that are too close to zero, so that taking logs
    of them does not produce infinite values. Use the EM.control argument to
    adjust epsilon.

    \code{constraints_sum_zero}: the chosen value of constraints_sum_zero.

    \code{param_lengths}: vector of total number of parameters/coefficients
    for each part of the model, labelled with the names of the components.
    The value is 0 for each component that is not included in the model, e.g.
    if there are no covariates interacting with row clusters then the
    \code{rowc_cov_coef} value will be 0. If the component is included, then
    the value given will include any dependent parameter/coefficient values,
    so if column clusters are included then the \code{colc_coef} value will
    be \code{nclus.column}, whereas the number of independent values will be
    \code{nclus.column - 1}.

    \code{initvect}: the initial \emph{vector} of parameter values, either
    specified by the user or generated automatically. This vector has only
    the \strong{independent} values of the parameters, not the full set.

    \code{outvect}: the final \emph{vector} of parameter values, containing
    only the independent parameter values from \code{parlist.out}.

    \code{parlist.init}: the initial list of parameters, constructed from
    the initial parameter vector \code{initvect}. Note that if the initial
    vector has been incorrectly specified, the values of \code{parlist.init}
    may not be as expected, and they should be checked by the user.

    \code{parlist.out}: fitted values of parameters.

    \code{pi}, \code{kappa}: fitted values of pi and kappa, where relevant.

    \code{ppr}, \code{ppc}: the posterior probabilities of membership of the
    row clusters and the column clusters, where relevant.

    \code{rowc_mm}, \code{colc_mm}, \code{cov_mm}: the model matrices for,
    respectively, the covariates interacting with row clusters, the covariates
    interacting with column clusters, and the covariates not interacting with
    row or column clusters (i.e. the covariates with constant coefficients).
    Note that one row of each model matrix corresponds to one row of long.df.

    \code{RowClusters}, \code{ColumnClusters}: the assigned row and column
    clusters, where relevant, where each row/column is assigned to a cluster
    based on maximum posterior probability of cluster membership (\code{ppr}
    and \code{ppc}).
}
\description{
Likelihood-based clustering with parameters fitted using the EM algorithm.
You can perform clustering on rows or columns of a data matrix, or biclustering
on both rows and columns simultaneously. You can include any number of
covariates for rows and covariates for columns.
Ordinal models used in the package are Ordered Stereotype Model (OSM),
Proportional Odds Model (POM) and a dedicated Binary Model for binary data.
}
\details{

}
\examples{
long.df <- data.frame(Y=factor(sample(1:3,5*20,replace=TRUE)),
               ROW=factor(rep(1:20,times=5)),COL=rep(1:5,each=20))

# Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*rowc_coef_r with 3 row clustering groups:
clustord.fit(Y~ROWCLUST,model="OSM",3,long.df=long.df)

# Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*(rowc_coef_r + col_coef_j) with 3 row clustering groups:
clustord.fit(Y~ROWCLUST+COL,model="OSM",3,long.df=long.df)

# Model Logit(P(Y <= k))=mu_k-rowc_coef_r-col_coef_j-rowc_col_coef_rj with 2 row clustering groups:
clustord.fit(Y~ROWCLUST*COL,model="POM",nclus.row=2,long.df=long.df)

# Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*(colc_coef_c) with 3 column clustering groups:
clustord.fit(Y~COLCLUST,model="OSM",nclus.column=3,long.df=long.df)

# Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*(colc_coef_c + row_coef_i) with 3 column clustering groups:
clustord.fit(Y~COLCLUST+ROW,model="OSM",nclus.column=3,long.df=long.df)

# Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*(rowc_coef_r + colc_coef_c)
#    with 3 row clustering groups and 2 column clustering groups:
clustord.fit(Y~ROWCLUST+COLCLUST,model="OSM",nclus.row=3,nclus.column=2,long.df=long.df,
             EM.control=list(EMcycles=5), nstarts=1)

# Model Logit(P(Y<=k))=mu_k-rowc_coef_r-colc_coef_c-rowc_colc_coef_rc
#    with 2 row clustering groups and 4 column clustering groups, and
#    interactions between them:
clustord.fit(Y~ROWCLUST*COLCLUST, model="POM", nclus.row=2, nclus.column=4,
             long.df=long.df,EM.control=list(EMcycles=5), nstarts=1,
             start_from_simple_model=FALSE)
}
\references{
Fernandez, D., Arnold, R., & Pledger, S. (2016). Mixture-based clustering for the ordered stereotype model. *Computational Statistics & Data Analysis*, 93, 46-75.
Anderson, J. A. (1984). Regression and ordered categorical variables. *Journal of the Royal Statistical Society: Series B (Methodological)*, 46(1), 1-22.
Agresti, A. (2010). *Analysis of ordinal categorical data* (Vol. 656). John Wiley & Sons.
}
