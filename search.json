[{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Regression with the Ordered Stereotype Model (OSM)","text":"clustord package primarily designed clustering ordinal data, can also use carry regression ordinal response data using Ordered Stereotype Model (OSM) using osm() function. proportional odds model (POM) ordinal model available clustering clustord, regression function provided , MASS package already contains function, polr(). osm() function clustord designed similar polr() function, similar arguments outputs. order demonstrate use osm() function, first load arthritis dataset multgee package. dataset contains self-assessment scores rheumatoid arthritis sufferers initial baseline assessment several follow-times. y variable self-assessment score, ordinal response. convert factor, necessary analysis. Since response already encoded values 1 5, need change default factor levels.","code":"library(clustord) library(multgee) head(arthritis) #>   id y sex age trt baseline time #> 1  1 4   2  54   2        2    1 #> 2  1 5   2  54   2        2    3 #> 3  1 5   2  54   2        2    5 #> 4  2 4   1  41   1        3    1 #> 5  2 4   1  41   1        3    3 #> 6  2 4   1  41   1        3    5 arthritis$y <- factor(arthritis$y)"},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"ordered-stereotype-model-definition","dir":"Articles","previous_headings":"","what":"Ordered Stereotype Model definition","title":"Regression with the Ordered Stereotype Model (OSM)","text":"define ordered stereotype model observation ii response value YiY_i qq levels indexed k=1,…,qk = 1, \\dots, q. YiY_i depends covariates 𝐗i\\bm{X}_i. ordered sterotype model takes form log(P(Yi=k|𝐱i)P(Yi=1|𝐱i))=μk+ϕk𝛃T𝐱i \\log\\left(\\dfrac{P(Y_i = k \\,|\\, \\bm{x}_i)}{P(Y_i = 1 \\,|\\, \\bm{x}_i)} \\right) = \\mu_k + \\phi_k \\bm{\\beta}^T\\bm{x}_i  μ1\\mu_1 fixed 0 identifiability 0=ϕ1≤…≤ϕk≤…≤ϕq=10 = \\phi_1 \\leq \\ldots \\leq \\phi_k \\leq \\ldots \\leq \\phi_q = 1. restriction ϕk\\phi_k values makes ordered stereotype model. 𝛃\\bm{\\beta} values coefficients covariates, linear regression model, except change probability YiY_i taking different levels. single coefficient positive, increases probability YiY_i taking higher levels, negative increases probability YiY_i taking lower levels. possible flexible model separate 𝛃\\bm{\\beta} coefficients level response, available package. Note cumulative model, logistic model, log\\log function left hand side model equation logit. contrast, proportional odds model cumulative logit model: log(P(Yi≤k|𝐱i)P(Yi>k|𝐱i))=μk−𝛃T𝐱i \\log\\left(\\dfrac{P(Y_i \\leq k \\,|\\, \\bm{x}_i)}{P(Y_i > k \\,|\\, \\bm{x}_i)} \\right) = \\mu_k - \\bm{\\beta}^T\\bm{x}_i Note minus sign form proportional odds model ensures coefficients 𝛃\\bm{\\beta} effect ordered stereotype model, .e. positive values coefficients increase probability YiY_i taking higher levels. ordered stereotype model flexible proportional odds model partly ϕk\\phi_k score parameters change effect coefficients different levels response, model proportional every response level. also flexible cumulative model, model based separate relative probability upper levels compared baseline reference level. , ratio log\\log part ratio level kk level 1, without reference levels. Therefore, data fit proportionality requirements proportional odds model, fitting ordered stereotype model provides flexibility increases number independent parameters q−2q - 2 (ϕ1\\phi_1 ϕq\\phi_q already fixed 0 1).","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"ordered-stereotype-model-fitting","dir":"Articles","previous_headings":"","what":"Ordered Stereotype Model fitting","title":"Regression with the Ordered Stereotype Model (OSM)","text":"can fit ordered stereotype model using standard regression call, specifying required formula dataset use. test model response, .e. self-assessment score, dependent baseline self-assessment score patient’s sex age. Since dataset multiple rows patient, containing self-assessment scores different follow-time points (.e. longitudinal dataset), select one time points analysis, using subset argument osm() function. Notice output initial fit includes warning message: Warning: converge iteration limit reached. warning indicates optim() function inside osm(), used fit model, manage converge solution. therefore need refit model allowing optim() run iterations, ensure convergence. can passing control argument optim(), list control parameters optim(). control parameter change upper limit number iterations, maxit. default 100, increase 5000. Now got converged solution, can now move interpreting results.","code":"fit <- osm(y ~ baseline + sex + age, data=arthritis, subset = (time == 1)) fit #> Call: #> osm(formula = y ~ baseline + sex + age, data = arthritis, subset = (time ==  #>     1)) #>  #> Coefficients beta: #>    baseline         sex         age  #>  2.64417662  0.57117223 -0.04976695  #>  #> Intercepts mu: #>                   1|2        1|3        1|4        1|5  #>  0.0000000  1.1060667  1.7175986 -0.2041696 -5.7546484  #>  #> Score parameters phi: #>         1         2         3         4         5  #> 0.0000000 0.2483075 0.3010923 0.5657090 1.0000000  #>  #> Residual Deviance: 729.4885  #> AIC: 749.4885  #> BIC: 786.493  #> (3 observations deleted due to missingness) #> Warning: did not converge as iteration limit reached fit <- osm(y ~ baseline + sex + age, data=arthritis,             subset = (time == 1), control=list(maxit=5000)) fit #> Call: #> osm(formula = y ~ baseline + sex + age, data = arthritis, control = list(maxit = 5000),  #>     subset = (time == 1)) #>  #> Coefficients beta: #>    baseline         sex         age  #>  1.86343731  0.81055028 -0.00908089  #>  #> Intercepts mu: #>                 1|2       1|3       1|4       1|5  #>  0.000000  1.419893  1.098219 -1.892299 -6.396478  #>  #> Score parameters phi: #>          1          2          3          4          5  #> 0.00000000 0.02112644 0.23847328 0.65444374 1.00000000  #>  #> Residual Deviance: 719.6567  #> AIC: 739.6567  #> BIC: 776.6611  #> (3 observations deleted due to missingness)"},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"covariate-coefficients","dir":"Articles","previous_headings":"Ordered Stereotype Model fitting","what":"Covariate coefficients","title":"Regression with the Ordered Stereotype Model (OSM)","text":"beta estimates estimated coefficients covariates baseline, sex age. positive values correspond higher probability getting high response levels (.e. case, ’re likely get y = 4 5 1 2), negative values correspond higher probability getting low response levels (.e. ’re likely get y = 1 2 4 5). see coefficient baseline 1.94, indicates baseline score individual big impact follow-self assessment score. baseline increases one level, makes likely follow-self-assessment score, y, also higher. see coefficient sex 0.84. reference level, 1, corresponds female, 2 corresponds male. male patients likely high self-assessment scores female patients. effect sex dramatic effect baseline score. Finally, see coefficient age -0.0090. much smaller effects baseline sex. ’s also slightly negative. indicates every additional year patient lived start trial reduced chances high follow scores, tiny bit. Overall, effect sizes suggest baseline sex relevant, age particularly important.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"mu-parameters","dir":"Articles","previous_headings":"Ordered Stereotype Model fitting","what":"mu parameters","title":"Regression with the Ordered Stereotype Model (OSM)","text":"mu estimates output , roughly speaking, intercept terms level response. intercept term amongst covariate coefficients: already incorporated mu parameters. need interpret estimates.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"phi-parameters","dir":"Articles","previous_headings":"Ordered Stereotype Model fitting","what":"phi parameters","title":"Regression with the Ordered Stereotype Model (OSM)","text":"phi estimates output “score” parameters unique Ordered Stereotype Model. first last values fixed 0 1. values remaining coefficients ϕ2=0.13\\phi_2 = 0.13, ϕ3=0.18\\phi_3 = 0.18, ϕ4=0.60\\phi_4 = 0.60. Note firstly ϕ2\\phi_2 ϕ3\\phi_3 similar estimated values. ϕk\\phi_k values modify effects covariates slightly different effects level. Since ϕ2\\phi_2 ϕ3\\phi_3 similar, covariates similar effects levels. Roughly speaking, means baseline pushes chances getting level 2 scores compared reference level 1, baseline also push chances getting level 3 scores compared reference level 1. ’s little difference pattern results response level 2 compared response level 3. tells us want simplify scores merge levels 2 3 without changing information results much. Let’s try .","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"merging-response-levels","dir":"Articles","previous_headings":"Ordered Stereotype Model fitting > phi parameters","what":"Merging response levels","title":"Regression with the Ordered Stereotype Model (OSM)","text":"start constructing new version response levels converted level “2.5” defining new factor: can see, coefficient estimates covariates changed little, general pattern , levels 2 3 giving us much different information covariates anyway. wanted simplify data even consider merging response levels 1 2.5, still fairly close together, ’s probably unnecessary. ϕ4\\phi_4 different ϕ2.5\\phi_{2.5} ϕ5\\phi_5, suggests three levels useful understanding effects covariates, merge . Generally speaking, consider difference consecutive ϕk\\phi_k values ’s smaller 0.1 small difference, difference 0.1 0.2 fairly small difference difference 0.2 large difference. Levels ϕk\\phi_k values 0.2 apart merged. merged levels 4 5 ’d expect results change lot:","code":"arthritis$y_merged <- as.numeric(as.character(arthritis$y)) arthritis$y_merged[arthritis$y_merged %in% c(2,3)] <- 2.5 arthritis$y_merged <- factor(arthritis$y_merged)  fit_merged <- osm(y_merged ~ baseline + sex + age, data=arthritis,                    subset = (time == 1), control = list(maxit = 5000)) fit_merged #> Call: #> osm(formula = y_merged ~ baseline + sex + age, data = arthritis,  #>     control = list(maxit = 5000), subset = (time == 1)) #>  #> Coefficients beta: #>   baseline        sex        age  #>  1.5711713  0.7145544 -0.0218884  #>  #> Intercepts mu: #>                 1|2.5        1|4        1|5  #>  0.0000000  2.7789813 -0.4433891 -4.9167416  #>  #> Score parameters phi: #>           1         2.5           4           5  #> 0.000000000 0.002477747 0.550989350 1.000000000  #>  #> Residual Deviance: 511.3156  #> AIC: 527.3156  #> BIC: 556.9191  #> (3 observations deleted due to missingness) arthritis$y_merged2 <- as.numeric(as.character(arthritis$y_merged)) arthritis$y_merged2[arthritis$y_merged2 %in% c(4,5)] <- 4.5 arthritis$y_merged2 <- factor(arthritis$y_merged2)  fit_merged2 <- osm(y_merged2 ~ baseline + sex + age, data=arthritis,                     subset = (time == 1), control = list(maxit = 5000)) fit_merged2 #> Call: #> osm(formula = y_merged2 ~ baseline + sex + age, data = arthritis,  #>     control = list(maxit = 5000), subset = (time == 1)) #>  #> Coefficients beta: #>     baseline          sex          age  #>  1.186768036  0.601354090 -0.009104307  #>  #> Intercepts mu: #>               1|2.5     1|4.5  #>  0.000000  2.037605 -1.695699  #>  #> Score parameters phi: #>        1      2.5      4.5  #> 0.000000 0.208971 1.000000  #>  #> Residual Deviance: 430.9078  #> AIC: 442.9078  #> BIC: 465.1104  #> (3 observations deleted due to missingness)"},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"recoding-the-response-levels-using-the-score-parameters","dir":"Articles","previous_headings":"Ordered Stereotype Model fitting > phi parameters","what":"Recoding the response levels using the score parameters","title":"Regression with the Ordered Stereotype Model (OSM)","text":"useful aspect ϕk\\phi_k parameters, apart indication levels response merged without much loss information, can used recode ordinal levels empirical way. , want apply numerical methods ordinal response variable, instead numbering levels 1, 2, etc. can number using ϕk\\phi_k values, empirically selected based data. wish, can rescale ϕk\\phi_k values levels range 1 qq, qq total number levels response. simply calculate vk=1+ϕk(q−1)v_k = 1 + \\phi_k (q-1) values vkv_k new codings response levels. Fernández, et al. (2021) demonstrates similar approach ordered stereotype scores used codings response levels applying numerical method, archetypoid analysis, recoded responses. However, multivariate analysis method, particular case clustering form ordered stereotype model fitted, instead regression model. Note recoding merely suggestion data, requirement. good priori reasons keeping response levels unmerged, wanting retain consistency analysis another related dataset, need merge levels. results simply indicate levels safely merged merging desirable.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"odds-ratios","dir":"Articles","previous_headings":"","what":"Odds ratios","title":"Regression with the Ordered Stereotype Model (OSM)","text":"Ordered Stereotype Model, possible calculate single odds ratio estimate covariate coefficient. model purposely constructed covariates can slightly different effects different levels response variable. overall effect covariate depends coefficient also ϕk\\phi_k parameter level kk response. implication single number, akin odds ratio, summarize covariate changes response. can say effect sizes bigger smaller, positive negative, convenient way summarize differences caused covariate using one single number.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"stereotype-model-functions-in-other-packages","dir":"Articles","previous_headings":"","what":"Stereotype Model functions in other packages","title":"Regression with the Ordered Stereotype Model (OSM)","text":"Note ordinalgmifs package also provides fitting function Stereotype Model. However, package fits Stereotype Model rather Ordered Stereotype Model. Ordered Stereotype Model restriction 0=ϕ1≤ϕ2≤…≤ϕq=10 = \\phi_1 \\leq \\phi_2 \\leq \\dots \\leq \\phi_q = 1, qq number levels response variable, restriction enforces ordinal nature response. Stereotype Model, .e. model fitted ordinalgmifs, restriction ϕk\\phi_k values.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/RegressionwithOSM.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Regression with the Ordered Stereotype Model (OSM)","text":"Fernández, D., Epifanio, . McMillan, L. F. (2021) Archetypal analysis ordinal data. Information Sciences, 579, pp. 281–292, https://doi.org/10.1016/j.ins.2021.07.095.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordConvergence.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"`clustord Convergence Tutorial","text":"clustering algorithm clustord based EM algorithm. iterative algorithm finding maximum likelihood estimates. possible predict advance many iterations algorithm require converge solution given dataset clustering settings. stop running specified number iterations. user can change value, defaults middlingly low number (50) order prevent algorithm running long typical run. important always check whether algorithm converged end given run.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordConvergence.html","id":"how-to-check-convergence","dir":"Articles","previous_headings":"","what":"How to check convergence","title":"`clustord Convergence Tutorial","text":"Let’s use simulated dataset demonstrate check convergence using simplest row clustering model. can check convergence three ways. Display results object: Display summary results: Fetch convergence check results object: convergence shown TRUE algorithm converged, FALSE . output completion algorithm run also display message indicating whether converged.","code":"library(clustord) set.seed(30) long.df.sim <- data.frame(Y=factor(sample(1:3,5*30,replace=TRUE)),                           ROW=factor(rep(1:30,times=5)),COL=rep(1:5,each=30)) fit <- clustord(Y ~ ROWCLUST, model=\"POM\", nclus.row=2, long.df=long.df.sim,                  nstarts=2, EM.control = list(EMcycles=2)) #> [1] \"Converting factor ROW to numeric.\" #> [1] \"EM algorithm for POM\" #> Randomly generated start #1 #> Found better incomplete log-like: -164.97238416875  #> Randomly generated start #2 #> Found better incomplete log-like: -164.81282792906  #> [1] \"EM algorithm has not converged. Please try again, or with a different random seed, or with more starting points.\" fit #>  #> Call: #> clustord(formula = Y ~ ROWCLUST, model = \"POM\", nclus.row = 2,  #>     long.df = long.df.sim, EM.control = list(EMcycles = 2), nstarts = 2) #>  #> Clustering mode: #>  row clustering #>  #> Converged: #>  FALSE #>  #> Cluster sizes: #> Row clusters:  30 0 summary(fit) #>  #> Call: #> clustord(formula = Y ~ ROWCLUST, model = \"POM\", nclus.row = 2,  #>     long.df = long.df.sim, EM.control = list(EMcycles = 2), nstarts = 2) #>  #> Clustering mode: #>  row clustering #>  #> Converged: #>  FALSE #> AIC: 337.5446  #>  #> BIC: 349.5871  #>  #>  #> Cluster sizes: #> Row clusters:  30 0  #>  #> Parameter estimates: #> $mu #>    mu_1    mu_2  #> -0.5103  0.9408  #>  #> $rowc #>  rowc_1  rowc_2  #>  0.2454 -0.2454 fit$EM.status$converged #> [1] FALSE"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordConvergence.html","id":"why-is-convergence-important","dir":"Articles","previous_headings":"","what":"Why is convergence important?","title":"`clustord Convergence Tutorial","text":"algorithm converged, means found optimum solution. Therefore, parameter estimates accurate cluster memberships may accurate either.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordConvergence.html","id":"what-to-do-if-the-algorithm-has-not-converged","dir":"Articles","previous_headings":"","what":"What to do if the algorithm has not converged","title":"`clustord Convergence Tutorial","text":"algorithm converged, best solution keep running stopped, converge. , start algorithm scratch, use output run stopped without converging input new run. order , use rerun() function. allows pass previous run use starting point new run. supply output previous run, dataset, options want EM.control, may include changing number iterations. diagnostic purposes, also option changing verbose setting new run supplying different optim.control list. optim.control entry called trace can make optim() produce much verbose output M-step, users need functionality. now, just ask iterations next run using EM.control since previous run used 5.","code":"fit_continued <- rerun(fit, long.df=long.df.sim, EM.control=list(EMcycles=20)) #> [1] \"Converting factor ROW to numeric.\" #> [1] \"EM algorithm for POM\" #> [1] \"EM algorithm has successfully converged.\" summary(fit_continued) #>  #> Call: #> clustord(formula = formula, model = model, nclus.row = nclus.row,  #>     nclus.column = nclus.column, long.df = long.df, initvect = initvect,  #>     pi.init = pi.init, kappa.init = kappa.init, EM.control = EM.control,  #>     optim.control = optim.control, verbose = verbose) #>  #> Clustering mode: #>  row clustering #>  #> Converged: #>  TRUE #> AIC: 337.4933  #>  #> BIC: 349.5358  #>  #>  #> Cluster sizes: #> Row clusters:  30 0  #>  #> Parameter estimates: #> $mu #>    mu_1    mu_2  #> -0.5740  0.8747  #>  #> $rowc #>  rowc_1  rowc_2  #>  0.1696 -0.1696"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordConvergence.html","id":"what-if-it-still-doesnt-converge","dir":"Articles","previous_headings":"","what":"What if it still doesn’t converge?","title":"`clustord Convergence Tutorial","text":"still find algorithm converge within e.g. 200 iterations, worth running algorithm scratch (.e. without supplying initvect) increasing number starts order improve chances finding better starting point might allow algorithm converge faster. use least 20 starting points clsutering structure complex Y ~ ROWCLUST (simplest model).","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordOrdinalModels.html","id":"tldr","dir":"Articles","previous_headings":"","what":"TL:DR","title":"`clustord` Ordinal Models","text":"proportional-odds model named \"POM\" clustord() model argument. commonly-used model ordinal data analysis, simplest, fewest parameters consistent pattern categories ordinal response variables. ordered stereotype model named \"OSM\" clustord() model argument. Compared proportional-odds model, one additional parameter every category/level ordinal response variable. additional parameters, plus non-cumulative structure, make much flexible. Use OSM think ordinal data may heterogeneous terms patterns different categories response variables. 𝚙𝚑𝚒k\\texttt{phi}_k score parameters response categories kk indicate much information available response category. 𝚙𝚑𝚒k\\texttt{phi}_k 𝚙𝚑𝚒k+1\\texttt{phi}_{k+1} similar (within 0.1 ) indicates response categories kk k+1k+1 provide much information clustering structure, means simplify data combining two response categories without much effect results analysis. can also use 𝚙𝚑𝚒k\\texttt{phi}_k scores OSM data-driven numerical encoding ordinal response categories better simply numbering categories 1,2,…,q1, 2, \\ldots, q carry analysis using methods numerical data, k-means (Lloyd, 1982 MacQueen, 1967).","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordOrdinalModels.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"`clustord` Ordinal Models","text":"package, assume dataset ordinal data. common form survey data, might get asking participants ask series questions Likert-scale answers (example, ranking 1 = “Strongly Disagree” 5 = “Strongly Agree”).  refer data matrix 𝐘\\bm{Y}. index rows data matrix ii columns data matrix jj, individual response value defined YijY_{ij}. qq categories response variable YijY_{ij} indexed kk, k=1,…,qk = 1, \\dots, q. three broad types clustering: row clustering, column clustering biclustering. Within , multiple possible clustering structures. discussed detail clustord Tutorial vignette, summarised Clustering Structure Summary vignette. row clusters, indexed rr column clusters indexed cc. vignette discusses two types ordinal models available clustord: proportional-odds model (POM) ordered stereotype model (OSM).","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordOrdinalModels.html","id":"proportional-odds-model-pom","dir":"Articles","previous_headings":"","what":"Proportional-odds model (POM)","title":"`clustord` Ordinal Models","text":"first model proportional-odds model (Agresti, 2000). widely-used ordinal model, simplest. model easily recognisable regression model: logitP(Y≤k|𝐱)=log(P(Y≤k|𝐱)P(Y>k|𝐱))=μk−𝛃T𝐱fork=1,…,q−1 \\mbox{logit}P(Y \\leq k \\,|\\, \\bm{x}) = \\log\\left(\\dfrac{P(Y \\leq k \\,|\\, \\bm{x})}{P(Y > k \\,|\\, \\bm{x})}\\right) = \\mu_k - \\bm{\\beta}^T\\bm{x} \\; \\; \\mbox{}\\; k = 1, \\dots, q-1  μk\\mu_k intercept parameter response category kk 𝛃\\bm{\\beta} coefficients controlling effect 𝐱\\bm{x} response, YY. model named “proportional-odds” coefficients depend response category, kk. , effect 𝐱\\bm{x} every single category response. Thus, number coefficients one number covariates. also cumulative ordinal model expressed probability obtaining given response category lower, relative probability getting higher response categories. Along coefficients staying categories, part model enforces similar patterns effects every response category. proportional-odds clustering forms clustord introduced Matechou et al. (2016). Considering cell (,j)(,j) data matrix responses, row ii row cluster rr /column jj column cc model general shape: log(P(Yij≤k|∈r,j∈c)P(Yij>k|∈r,j∈c))=μk−ηrcijfork=1,…,q−1 \\log\\left(\\dfrac{P(Y_{ij} \\leq k \\,|\\, \\r,\\; j \\c)}{P(Y_{ij} > k \\,|\\, \\r,\\; j \\c)}\\right) = \\mu_k - \\eta_{rcij} \\; \\; \\mbox{}\\; k = 1, \\dots, q-1  μk\\mu_k parameter controls default probabilities different response categories absence clustering, ηrcij\\eta_{rcij} remaining part linear predictor. ηrcij\\eta_{rcij} part model determines clustering structure, proportional-odds model ordered stereotype model within clustord. ηrcij\\eta_{rcij} subtracted μk\\mu_k, rather added, parameter within ηrcij\\eta_{rcij} positive corresponds higher probability obtaining higher response categories (whereas added, positive effects lead higher probability obtaining lower response categories). logit notation compact way expressing model, alternatively can express terms probability, θijk\\theta_{ijk}, getting single response category, kk, cell YijY_{ij}: θijk|∈r,j∈c={exp(μk−ηrcij)1+exp(μk−ηrcij)k=1exp(μk−ηrcij)1+exp(μk−ηrcij)−exp(μk−1−ηrcij)1+exp(μk−1−ηrcij)1<k<q1−∑k=1q−1θijkk=q \\theta_{ijk} \\,|\\, \\r,\\;j \\c =  \\begin{cases} \\dfrac{\\exp(\\mu_k - \\eta_{rcij})}{1+\\exp(\\mu_k - \\eta_{rcij})} & k = 1\\\\[10pt] \\dfrac{\\exp(\\mu_k - \\eta_{rcij})}{1+\\exp(\\mu_k - \\eta_{rcij})} - \\dfrac{\\exp(\\mu_{k-1} - \\eta_{rcij})}{1+\\exp(\\mu_{k-1} - \\eta_{rcij})} & 1 < k < q \\\\[10pt] 1 - \\sum_{k=1}^{q-1} \\theta_{ijk} & k = q \\end{cases} potential clustering structures within clustord, expressed POM form: Column clustering biclustering can similarly include covariates, way row clustering can. Note coefficient/covariate structure including covariates clustering models regardless whether covariates attached rows (𝐱i\\bm{x}_i) columns (𝐰j\\bm{w}_j), clustord package combines coefficients one single cov parameter vector, order coefficients corresponding order ’re included formula provided clustord().","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordOrdinalModels.html","id":"ordered-stereotype-model","dir":"Articles","previous_headings":"","what":"Ordered stereotype model","title":"`clustord` Ordinal Models","text":"second model ordered stereotype model (introduced Anderson, 1984 described Agresti, 2000). flexible model proportional-odds model. one additional set parameters, {ϕk}\\{\\phi_k\\}, non-cumulative logit structure. regression model form: log(P(Y=k|𝐱)P(Y=1|𝐱))=μk+ϕk𝛃T𝐱fork=2,…,q \\log\\left(\\dfrac{P(Y = k \\,|\\, \\bm{x})}{P(Y = 1 \\,|\\, \\bm{x})}\\right) = \\mu_k + \\phi_k\\bm{\\beta}^T\\bm{x} \\; \\; \\mbox{}\\; k = 2, \\dots, q  𝛃\\bm{\\beta} coefficients controlling effect 𝐱\\bm{x} response, YY. μ1\\mu_1 set 0 ensure identifiability ϕk\\phi_k parameters constrained ordered: 0=ϕ1≤ϕ2≤…≤ϕq=10 = \\phi_1 \\leq \\phi_2 \\leq \\dots \\leq \\phi_q = 1. (non-ordered stereotype model lacks constraint, can used model nominal data.) ordered stereotype model, ϕk\\phi_k parameters modify effect covariate response effect varies response categories. Moreover, model non-cumulative, pattern response category 3, relative category 1, can different pattern response category 2, relative category 1. ordered stereotype clustering forms clustord defined Fernández et al. (2016) Fernández et al. (2019). cell (,j)(,j) data matrix responses, row ii row cluster rr /column jj column cc model general shape: log(P(Yij=k|∈r,j∈c)P(Yij=1|∈r,j∈c))=μk+ϕkηrcijfork=2,…,q \\log\\left(\\dfrac{P(Y_{ij} = k \\,|\\, \\r,\\; j \\c)}{P(Y_{ij} = 1 \\,|\\, \\r,\\; j \\c)}\\right) = \\mu_k + \\phi_k\\eta_{rcij} \\; \\; \\mbox{}\\; k = 2, \\dots, q  μk\\mu_k parameter controls default probabilities different response categories absence clustering, ϕk\\phi_k score parameter category kk ηrcij\\eta_{rcij} remaining part linear predictor. ηrcij\\eta_{rcij} proportional-odds model. POM, notation compact way expressing OSM, alternatively can express terms probability, θijk\\theta_{ijk}, getting single response category, kk, cell YijY_{ij}: θijk|∈r,j∈c=exp(μk+ϕkηrcij)∑l=1qexp(μl+ϕlηrcij)k=1,…,q \\theta_{ijk} \\,|\\, \\r,\\;j \\c =  \\dfrac{\\exp(\\mu_k + \\phi_k\\eta_{rcij})}{\\sum_{l=1}^q \\exp(\\mu_l + \\phi_l \\eta_{rcij})} k = 1, \\dots, q potential clustering structures within clustord, expressed OSM form: POM, coefficient/covariate structure including covariates clustering models regardless whether covariates attached rows (𝐱i\\bm{x}_i) columns (𝐰j\\bm{w}_j), clustord package combines coefficients one single cov parameter vector, order coefficients corresponding order ’re included formula provided clustord(). Note ηrcij\\eta_{rcij} takes forms POM OSM even though overall distribution shapes differ.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordOrdinalModels.html","id":"a-note-about-notation","dir":"Articles","previous_headings":"","what":"A note about notation","title":"`clustord` Ordinal Models","text":"looking cited journal articles Pledger Arnold (2014), Matechou et al. (2016), Fernández et al. (2016 2019), notation slightly different notation used tutorial. package tutorial notation changed reduce confusion parameters row clustering column clustering models. Table 1 glossary notation used clustord corresponding notation used articles. rest parameters retain names tutorial cited references. Note also , although theoretically possible model structure add αr\\alpha_r αi\\alpha_i model, ie. row cluster effects individual row effects, clustord allow , warn try use Y ~ ROWCLUST + ROW similar formulae. biclustering model, αr\\alpha_r βc\\beta_c, allow either individual row individual column effects, partly introduce many parameters difficult fit correctly.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordOrdinalModels.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"`clustord` Ordinal Models","text":"Agresti, . (2010). Analysis ordinal categorical data. Vol. 656, John Wiley & Sons. Anderson, J. . (1984). Regression ordered categorical variables. Journal Royal Statistical Society – Series B (Methodological), pp. 1–30. Fernández, D., Arnold., R. Pledger, S. (2016). Mixture-based clustering ordered stereotype model. Computational Statistics & Data Analysis, 93, pp. 46–75. Fernández, D., Arnold, R., Pledger, S., Liu, ., & Costilla, R. (2019). Finite mixture biclustering discrete type multivariate data. Advances Data Analysis Classification, 13, pp. 117–143. Lloyd, S. P. (1982). Least squares quantization PCM. IEEE Transactions Information Theory, 28(2), pp. 129–137. MacQueen, J. B. (1967). Methods classification Analysis Multivariate Observations. Proceedings 5th Berkeley Symposium Mathematical Statistics Probability. University California Press, 1(14), pp. 281–297. Matechou, E., Liu, ., Fernández, D. Farias, M., Gjelsvik, B. (2016). Biclustering models two-mode ordinal data. Psychometrika, 81, pp. 611–624. Pledger, S. Arnold, R. (2014). Multivariate methods using mixtures: Correspondence analysis, scaling pattern-detection. Computational Statistics Data Analysis 71, pp. 241–261.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"row-clustering","dir":"Articles","previous_headings":"","what":"Row clustering","title":"`clustord` Structure Summary","text":"clustord package can cluster rows data matrix, often correspond subjects observations.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"row-clustering-only","dir":"Articles","previous_headings":"Row clustering","what":"Row clustering only","title":"`clustord` Structure Summary","text":"additional patterns data want incorporate, simplest form row clustering.  formula : Y ~ ROWCLUST Parameters: rowc","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"row-clustering-with-individual-column-effects","dir":"Articles","previous_headings":"Row clustering","what":"Row clustering with individual column effects","title":"`clustord` Structure Summary","text":"think individual column (e.g. survey question) slightly different patterns, can incorporate clustering structure.  formula without interaction: Y ~ ROWCLUST + COL formula interaction: Y ~ ROWCLUST*COL , example, gradient effects individual columns, interaction row cluster allow one cluster gradient going one direction cluster gradient going direction. Parameters: rowc, col possible interaction rowc_col.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"row-clustering-with-row-covariates","dir":"Articles","previous_headings":"Row clustering","what":"Row clustering with row covariates","title":"`clustord` Structure Summary","text":"additional information rows, demographic information survey respondent, think influence answered survey, can incorporate clustering structure:  many possible formulae , follow similar style formulae lm glm. Example formulae without interaction row clustering row covariates: Y ~ ROWCLUST + age Y ~ ROWCLUST + age + nationality*occupation Example formulae interaction row clustering row covariates: Y ~ ROWCLUST*age + nationality*occupation need also supply row covariates xr.df function mat2df() creating long data frame clustering (see clustord Tutorial vignette details). can also include individual column effects well covariates: Y ~ ROWCLUST*age + COL Parameters: rowc, cov possible interaction rowc_cov, possible column effects col.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"row-clustering-with-column-covariates","dir":"Articles","previous_headings":"Row clustering","what":"Row clustering with column covariates","title":"`clustord` Structure Summary","text":"additional information columns, probability particular question answered honestly, think influence response data, can incorporate clustering structure:  formula works covariates: Y + ROWCLUST + honesty need supply column covariates xc.df function mat2df() creating long form data frame.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"column-clustering","dir":"Articles","previous_headings":"","what":"Column clustering","title":"`clustord` Structure Summary","text":"clustord package can cluster columns data matrix, often correspond survey questions.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"column-clustering-only","dir":"Articles","previous_headings":"Column clustering","what":"Column clustering only","title":"`clustord` Structure Summary","text":"additional patterns data want incorporate, simplest form column clustering.  formula Y ~ COLCLUST Parameters: colc","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"column-clustering-with-individual-row-effects","dir":"Articles","previous_headings":"Column clustering","what":"Column clustering with individual row effects","title":"`clustord` Structure Summary","text":"think individual row (e.g. survey respondent) slightly different patterns, can incorporate clustering structure.  formula without interaction: Y ~ COLCLUST + ROW formula interaction: Y ~ COLCLUST*ROW , example, gradient effects individual respondents, interaction column cluster allow one cluster columns gradient respondents going one direction column cluster gradient respondents going direction. IMPORTANT: Many datasets far rows columns. case, e.g. number rows > 100, model may suitable fitted requires one parameter every row difficult fit many parameters accurately. Parameters: colc, row possible interaction colc_row.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"column-clustering-with-column-covariates","dir":"Articles","previous_headings":"Column clustering","what":"Column clustering with column covariates","title":"`clustord` Structure Summary","text":"additional information columns, probability particular question answered honestly, think influence response data, can incorporate clustering structure:  formula works covariates. Column clustering covariates without interaction: Y + COLCLUST + honesty Column clustering covariates interaction: Y + COLCLUST*honesty Supply column covariates xc.df mat2df(). Parameters: colc, cov possible interaction colc_cov.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"column-clustering-with-row-covariates","dir":"Articles","previous_headings":"Column clustering","what":"Column clustering with row covariates","title":"`clustord` Structure Summary","text":", additional information rows (e.g. survey respondents) think affect response values, can incorporate clustering columns:  formula works covariates. Column clustering row covariates without interaction: Y + COLCLUST + age Column clustering covariates interaction: Y + COLCLUST*age Supply row covariates xr.df mat2df(). Parameters: colc, cov possible interaction colc_cov.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"biclustering","dir":"Articles","previous_headings":"","what":"Biclustering","title":"`clustord` Structure Summary","text":"package can also cluster rows columns simultaneously, call biclustering. finds combinations subjects questions exhibit similar response patterns:  model can include just main row column clustering effects: Y ~ ROWCLUST + COLCLUST can also include interaction : Y ~ ROWCLUST*COLCLUST Parameters: rowc, colc possible interaction rowc_colc.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordStructureSummary.html","id":"biclustering-with-covariates","dir":"Articles","previous_headings":"Biclustering","what":"Biclustering with covariates","title":"`clustord` Structure Summary","text":"can include covariates performing biclustering:  Biclustering row column covariates without interaction: Y + ROWCLUST + COLCLUST + age + honesty Biclustering row column cluster interactions covariate interaction: Y + ROWCLUST*COLCLUST + age + honesty Biclustering without interaction row column clusters, interaction row clusters row covariates: Y ~ ROWCLUST*age + COLCLUST + honesty Biclustering row column cluster interactions interaction row clusters row covariates: Y ~ ROWCLUST*age + COLCLUST + ROWCLUST:COLCLUST + honesty Biclustering without interaction row column clusters, interaction column clusters column covariates: Y ~ ROWCLUST + age + COLCLUST*honesty Biclustering row column cluster interactions interaction column clusters column covariates: Y ~ ROWCLUST + COLCLUST*honesty + ROWCLUST:COLCLUST + age include three-way interactions row column cluster interaction: formula terms ROWCLUST:COLCLUST:age disallowed. Parameters: rowc colc, covariate effects cov, possible cluster interaction rowc_colc, possible cluster-covariate interactions rowc_cov colc_cov.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"tldr","dir":"Articles","previous_headings":"","what":"TL:DR","title":"`clustord` Tutorial","text":"clustord() function can perform row clustering, column clustering biclustering data matrix. formula argument works similarly ones lm() glm(), except uses four special keywords: ROWCLUST COLCLUST include row column clusters, ROW COL include individual row column effects. convert data matrix long-format clustering, using mat2df() function. clustering, perform model selection using AIC BIC criteria part output. Examine parameter estimates within parlist.part output. Positive parameter estimates increase chances getting higher ordinal responses, whereas negative parameter estimates increase chances getting lower ordinal responses. can include covariates clustering, can numerical categorical, just like predictors regression model. Add inputs mat2df() make object clustering. Use covariate names formula like lm() glm(). Check algorithm converged using EM.status$converged output object, , try increasing number random starting points using nstarts increase maximum number EM iterations using EM.control = list(EMcycles = X) input, X number iterations want. clustord can fit two kinds ordinal models. “POM”, proportional-odds model, simplest, widely used ordinal model. “OSM”, ordered stereotype model, flexible phi parameters can used informed way recoding ordinal data numerically. two models discussed separate vignette, “Ordinal Models”.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"`clustord` Tutorial","text":"package clustord uses function clustord() link finite-mixture clustering biclustering (Pledger Arnold, 2014) two ordinal models: proportional odds model ordered stereotype model (Agresti, 2010, Anderson, 1984). clustering models package highly flexible can incorporate distinct patterns subsets items within clusters, can also incorporate additional covariates. package, assume dataset ordinal data. common form survey data, might get asking participants ask series questions Likert-scale answers (example, ranking 1 = “Strongly Disagree” 5 = “Strongly Agree”).  clustord package can cluster rows data matrix, often correspond subjects observations:  package can cluster columns data matrix, often correspond survey questions:  Mathematically, two forms clustering equivalent, can orient data matrix either way round, just choose appropriate clustering direction. package can also cluster rows columns simultaneously, call biclustering. finds combinations subjects questions exhibit similar response patterns:","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"model-based-clustering","dir":"Articles","previous_headings":"Introduction","what":"Model-based clustering","title":"`clustord` Tutorial","text":"clustering algorithms package model-based clustering methods. models finite-mixture models, cluster assumed correspond particular statistical distribution.  Many common clustering methods, k-means (Lloyd, 1982 MacQueen, 1967) distance-based instead model-based.  Model-based clustering methods often take little longer set run, one major advantage. clustering methods estimate cluster item member . package, like finite-mixture methods, provides posterior probabilities cluster membership, given data. clusters assumed correspond statistical distributions, methods also provide parameter estimates statistical distributions. words, can obtain general information patterns exhibited clusters. statistical framework model-based clustering also allows carry goodness--fit tests perform model selection using common measures AIC BIC. package fits mixture models maximising likelihood using Expectation-Maximisation algorithm (Dempster, Laird & Rubin 1977, McLachlan Krishnan 2007). Many examples types models can found , e.g., McLachlan & Basford (1988) McLachlan & Peel (2000).","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"ordinal-data","dir":"Articles","previous_headings":"Introduction","what":"Ordinal data","title":"`clustord` Tutorial","text":"common approach clustering ordinal data number categories ordinal variable treat data continuous. allows use numerical clustering methods like k-means. encoding ordinal categories continuous encodes assumptions relative spacings ordinal categories. common approach number categories 1 qq:  often category labels dropped:  encoding assumes levels 1 2 close together levels 2 3. assumption necessarily accurate. example, people asked question much pain feeling, may bigger difference perception pain levels Moderate Severe (2 3) pain levels Mild Moderate (1 2):  top scale assumes levels equally spaced, bottom scale accurate representation. Rather treating ordinal data numerical applying continuous-data clustering algorithms, ordinal models package make assumptions numerical encoding ordinal categories, observe ranking categories. ordered stereotype model (OSM), one two ordinal models package goes : set model parameters, {phik}\\{phi_k\\}, can treated scores category levels. fitted values parameters can used scoring system accurately reflects spacings levels according data:  start five categories, fitted {ϕk}\\{\\phi_k\\} values levels 1 2 close together (e.g. 0 0.09), indicates almost different information provided levels 2 3. potentially combine two levels, simplify data without losing much information. ordinal models discussed detail Ordinal Models vignette.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"fitting-clustord-models","dir":"Articles","previous_headings":"","what":"Fitting clustord models","title":"`clustord` Tutorial","text":"","code":"library(clustord)"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"data-format","dir":"Articles","previous_headings":"Fitting clustord models","what":"Data format","title":"`clustord` Tutorial","text":"vignette, use simple survey example, data matrix matrix responses questions subjects rows questions columns. questions responses 1 7 – current version clustord set handle datasets questions responses others. illustrations show tiny exemplar dataset. actual dataset analysis 82 rows (subjects) 12 columns (questions). refer data matrix 𝐘\\bm{Y}. index rows data matrix ii columns data matrix jj, individual response value defined YijY_{ij}.  can carry model-fitting clustord, need convert data long-form data frame instead data matrix. long form, used simplify inner implementation clustering process, one row per cell original data frame. Two columns labelled ROW COL indicate row column original data frame response value came :  cell data matrix missing data, entry included long-form data frame (1 cell 10x10 data matrix missing, long-form data frame 99 rows). long-form also incorporates covariates linked responses. discuss later. clustord provides function, mat2df(), carry conversion long form data frame: may construct long-form data frame want, minor restrictions: data frame MUST contain column labelled “Y” contains response values, column labelled “ROW” (case-sensitive) contains row names/numbers column labelled “COL” (case-sensitive) contains column names/numbers, rows missing cell values (NA missing indicators) deleted.","code":"df <- read.table(\"eval_survey.txt\") colnames(df) <- paste0(\"Q\", 1:ncol(df)) rownames(df) <- paste0(\"ID\", 1:nrow(df)) head(df) ##     Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 ## ID1  6  2  2  2  2  3  3  3  3   3   2   2 ## ID2  7  1  2  1  2  3  4  4  4   5   2   2 ## ID3  7  2  2  1  3  3  2  3  4   3   3   3 ## ID4  6  3  3  2  2  3  3  3  4   4   3   3 ## ID5  7  2  2  2  2  3  3  4  4   4   2   4 ## ID6  6  2  1  2  6  3  1  3  6   3  NA   3 dim(df) ## [1] 82 12 long.df <- mat2df(df) ## Warning in mat2df(df): Removing 4 entries for which Y is NA. head(long.df) ##   Y ROW COL ## 1 6   1   1 ## 2 7   2   1 ## 3 7   3   1 ## 4 6   4   1 ## 5 7   5   1 ## 6 6   6   1"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"clustord-models","dir":"Articles","previous_headings":"Fitting clustord models","what":"clustord models","title":"`clustord` Tutorial","text":"specific structure clustering model used page form proposed Pledger Arnold (2014). paper proposed models binary count data (see clustglm package Shirley Pledger), similar forms proposed proportional odds model Matechou et al. (2016) ordered stereotype model Fernández et al. (2016, 2019). clustering models linear predictor structure. link linear predictor response values varies two different ordinal models, linear predictor structure , linear predictor structure used clustglm models. define νij\\nu_{ij} linear predictor response YijY_{ij}. various models outlined include different additive components linear predictor influence probabilities obtaining different response categories. call components “effects”. Regression models often described including “main effects” “interaction effects”, sometimes “random effects”, similarly clustering models primarily “cluster effects”. can also include “covariate effects” “individual row/column effects”.","code":""},{"path":[]},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"row-cluster-effect-only","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering","what":"Row cluster effect only","title":"`clustord` Tutorial","text":"describe possible row clustering structures first. individual row clusters indexed rr. basic row clustering model row cluster effects, assumes every response across columns row ii cluster rr probabilities different categories. main part linear predictor model “row cluster effect”, differs cluster cluster. row cluster effect parameters labelled rowc clustord() output.  console output, basic model described row-cluster-model. default, model used starting point models: starting points row cluster parameter estimates found fitting simpler model first fitting full forms complex models.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"fitting-the-model","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering > Row cluster effect only","what":"Fitting the model","title":"`clustord` Tutorial","text":"can fit model using clustord() function, main model-fitting function clustord. first input argument clustord() formula, gives formula model, just lm(), glm() similar functions. fit model using case-sensitive keyword ROWCLUST formula. left-hand side formula always Y. basic formula : next input argument character string indicating model, either \"POM\" proportional-odds model, \"OSM\" ordered-stereotype model, \"Binary\" binary model (proportional-odds ordered-stereotype). ’ll talk models detail later, now ’ll use proportional-odds model, widely-used simplest ordinal model. third fourth input arguments nclus.row nclus.column, used define number row /column clusters. row clustering, specify nclus.row choose fit 2 clusters. clustord can fit specified number clusters, later discuss select best number clusters. fifth input argument long.df, asking long form data frame prepared earlier. , leaving rest arguments default values, fit basic row clustering model dataset: also uses option verbose=FALSE displays reduced output progress algorithm. discuss meaning outputs section important algorithm settings. tutorial set random number seed running fitting process, order keep included output consistent. algorithm uses random selection starting points (see section important algorithm settings detail).","code":"Y ~ ROWCLUST set.seed(2) fit_rowclust_only <- clustord(Y ~ ROWCLUST,     \"POM\", nclus.row = 2, long.df = long.df,     verbose = FALSE)"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"checking-the-output","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering > Row cluster effect only","what":"Checking the output","title":"`clustord` Tutorial","text":"fit completed first check converged: can look probabilities cluster membership: instance, almost individuals firmly assigned one cluster cluster. can also look cluster memberships, obtained simple assignment process assigning individual cluster highest posterior probability membership (ways assign individuals clusters, implemented within clustord). cluster, vector row numbers cluster provided. instance, four individuals allocated second cluster. “mixing proportions”, πr\\pi_r, summarise proportion rows cluster (mixing proportion mean posterior probabilities membership cluster across rows). 5% rows second cluster, matches saw cluster memberships. can look parameter values cluster. mu values parameter list discussed later, section different ordinal models. now, just look row cluster effects, called rowc. output includes .init parameter values used start EM algorithm, .parameter values final ones end algorithm, .values ones want: Positive values row cluster parameters increase probability getting higher ordinal categories, negative values parameters increase probability getting lower ordinal categories. can see individuals first cluster tend provide higher-value responses individuals second cluster. can also check mean value responses individuals first second clusters:  discuss goodness--fit later section model selection.","code":"fit_rowclust_only$EM.status$converged ## [1] TRUE round(fit_rowclust_only$ppr, 2) ##       [,1] [,2] ##  [1,] 1.00 0.00 ##  [2,] 1.00 0.00 ##  [3,] 1.00 0.00 ##  [4,] 1.00 0.00 ##  [5,] 1.00 0.00 ##  [6,] 1.00 0.00 ##  [7,] 1.00 0.00 ##  [8,] 1.00 0.00 ##  [9,] 1.00 0.00 ## [10,] 1.00 0.00 ## [11,] 1.00 0.00 ## [12,] 0.96 0.04 ## [13,] 1.00 0.00 ## [14,] 1.00 0.00 ## [15,] 1.00 0.00 ## [16,] 1.00 0.00 ## [17,] 0.00 1.00 ## [18,] 1.00 0.00 ## [19,] 1.00 0.00 ## [20,] 1.00 0.00 ## [21,] 1.00 0.00 ## [22,] 1.00 0.00 ## [23,] 1.00 0.00 ## [24,] 1.00 0.00 ## [25,] 1.00 0.00 ## [26,] 1.00 0.00 ## [27,] 1.00 0.00 ## [28,] 1.00 0.00 ## [29,] 1.00 0.00 ## [30,] 1.00 0.00 ## [31,] 1.00 0.00 ## [32,] 1.00 0.00 ## [33,] 0.00 1.00 ## [34,] 1.00 0.00 ## [35,] 1.00 0.00 ## [36,] 1.00 0.00 ## [37,] 1.00 0.00 ## [38,] 0.94 0.06 ## [39,] 1.00 0.00 ## [40,] 1.00 0.00 ## [41,] 1.00 0.00 ## [42,] 1.00 0.00 ## [43,] 1.00 0.00 ## [44,] 1.00 0.00 ## [45,] 1.00 0.00 ## [46,] 1.00 0.00 ## [47,] 1.00 0.00 ## [48,] 1.00 0.00 ## [49,] 1.00 0.00 ## [50,] 1.00 0.00 ## [51,] 1.00 0.00 ## [52,] 1.00 0.00 ## [53,] 1.00 0.00 ## [54,] 1.00 0.00 ## [55,] 1.00 0.00 ## [56,] 1.00 0.00 ## [57,] 1.00 0.00 ## [58,] 1.00 0.00 ## [59,] 1.00 0.00 ## [60,] 1.00 0.00 ## [61,] 0.00 1.00 ## [62,] 1.00 0.00 ## [63,] 1.00 0.00 ## [64,] 1.00 0.00 ## [65,] 1.00 0.00 ## [66,] 1.00 0.00 ## [67,] 0.00 1.00 ## [68,] 1.00 0.00 ## [69,] 1.00 0.00 ## [70,] 1.00 0.00 ## [71,] 1.00 0.00 ## [72,] 1.00 0.00 ## [73,] 1.00 0.00 ## [74,] 1.00 0.00 ## [75,] 1.00 0.00 ## [76,] 1.00 0.00 ## [77,] 1.00 0.00 ## [78,] 1.00 0.00 ## [79,] 1.00 0.00 ## [80,] 1.00 0.00 ## [81,] 1.00 0.00 ## [82,] 1.00 0.00 fit_rowclust_only$RowClusterMembers ## [[1]] ##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24 25 26 ## [26] 27 28 29 30 31 32 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 ## [51] 53 54 55 56 57 58 59 60 62 63 64 65 66 68 69 70 71 72 73 74 75 76 77 78 79 ## [76] 80 81 82 ##  ## [[2]] ## [1] 17 33 61 67 round(fit_rowclust_only$pi.out, 2) ## [1] 0.95 0.05 fit_rowclust_only$parlist.out$rowc ##    rowc_1    rowc_2  ##  1.732987 -1.732987 boxplot(split(rowMeans(df), fit_rowclust_only$RowClusters),     \"Mean response values across all questions for each individual\",     names = c(\"Cluster 1\", \"Cluster 2\"))"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"row-clusters-with-individual-column-effects","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering","what":"Row clusters with individual column effects","title":"`clustord` Tutorial","text":"second, slightly complex row clustering model one incorporates row cluster effects also individual effects columns.  can see looking actual data necessary, can see responses Q1 tend much higher values responses questions. row-cluster-model treats columns repeated measures, appear reasonable assumption case. additive nature clustering models package allows add linear predictor effect individual columns. main part linear predictor model becomes rowc + col col individual effects columns.","code":"head(df) ##     Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 ## ID1  6  2  2  2  2  3  3  3  3   3   2   2 ## ID2  7  1  2  1  2  3  4  4  4   5   2   2 ## ID3  7  2  2  1  3  3  2  3  4   3   3   3 ## ID4  6  3  3  2  2  3  3  3  4   4   3   3 ## ID5  7  2  2  2  2  3  3  4  4   4   2   4 ## ID6  6  2  1  2  6  3  1  3  6   3  NA   3"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"fitting-the-model-1","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering > Row clusters with individual column effects","what":"Fitting the model","title":"`clustord` Tutorial","text":"fit model using case-sensitive keyword COL formula: can see, formula, just like linear predictor, builds components additively, just regression formula argument. Note , default, try fit complex model, clustord() generate starting values row cluster effect parameters fitting row-cluster-model first, fitting model column effects. fit model using model type , POM:","code":"Y ~ ROWCLUST + COL set.seed(3) fit_rowclust_cols <- clustord(Y ~ ROWCLUST +     COL, \"POM\", nclus.row = 2, long.df = long.df,     verbose = FALSE)"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"checking-the-output-1","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering > Row clusters with individual column effects","what":"Checking the output","title":"`clustord` Tutorial","text":", fit completed first check converged: can look probabilities cluster membership: case, can see individuals less firmly assigned one particular cluster case row-cluster-model. assign clusters based highest probability, let’s see lists cluster members: see allow columns different others, end evenly split pair clusters. Now let’s look parameter values cluster: fit, first cluster negative row cluster effect, .e. individuals tend provide lower category responses, second cluster positive row cluster effect, .e. individuals tend provide higher category responses. Note “order” clusters appears different fit previous one: row-cluster-fit positive cluster effect first cluster row-clusters--columns fit negative cluster effect first cluster. common, clustering phenomenon known “label-switching”. mathematical model cluster equivalent whichever order clusters , clustering algorithm can’t tell difference different orderings clusters. now, just know read anything fact order changed. , let’s check mean value responses individuals first second clusters:  can see although order clusters different , still clear differentiation typical responses Cluster 1 vs. Cluster 2. Now let’s also check column effect parameters: can see first column parameter, Q1, much higher others, matches observed data, .e. much higher response values column others. Q4 lowest value parameter, question tends get lower value responses others, effect dramatic Q1. parameter values column effects small medium, even just presence one two different columns good reason fit model column effects.","code":"fit_rowclust_cols$EM.status$converged ## [1] TRUE round(fit_rowclust_cols$ppr, 2) ##       [,1] [,2] ##  [1,] 0.18 0.82 ##  [2,] 0.01 0.99 ##  [3,] 0.00 1.00 ##  [4,] 0.00 1.00 ##  [5,] 0.00 1.00 ##  [6,] 0.05 0.95 ##  [7,] 0.00 1.00 ##  [8,] 0.13 0.87 ##  [9,] 0.54 0.46 ## [10,] 0.00 1.00 ## [11,] 0.03 0.97 ## [12,] 1.00 0.00 ## [13,] 0.00 1.00 ## [14,] 0.00 1.00 ## [15,] 0.99 0.01 ## [16,] 0.98 0.02 ## [17,] 1.00 0.00 ## [18,] 0.01 0.99 ## [19,] 0.01 0.99 ## [20,] 0.97 0.03 ## [21,] 0.17 0.83 ## [22,] 0.44 0.56 ## [23,] 0.01 0.99 ## [24,] 0.91 0.09 ## [25,] 0.93 0.07 ## [26,] 0.99 0.01 ## [27,] 0.99 0.01 ## [28,] 0.96 0.04 ## [29,] 0.01 0.99 ## [30,] 0.95 0.05 ## [31,] 0.01 0.99 ## [32,] 1.00 0.00 ## [33,] 1.00 0.00 ## [34,] 0.00 1.00 ## [35,] 0.00 1.00 ## [36,] 1.00 0.00 ## [37,] 0.99 0.01 ## [38,] 1.00 0.00 ## [39,] 0.01 0.99 ## [40,] 0.00 1.00 ## [41,] 0.01 0.99 ## [42,] 0.10 0.90 ## [43,] 0.14 0.86 ## [44,] 0.96 0.04 ## [45,] 0.77 0.23 ## [46,] 0.00 1.00 ## [47,] 0.96 0.04 ## [48,] 0.96 0.04 ## [49,] 0.13 0.87 ## [50,] 0.00 1.00 ## [51,] 0.01 0.99 ## [52,] 0.00 1.00 ## [53,] 0.01 0.99 ## [54,] 0.56 0.44 ## [55,] 0.97 0.03 ## [56,] 0.15 0.85 ## [57,] 0.96 0.04 ## [58,] 0.00 1.00 ## [59,] 0.03 0.97 ## [60,] 0.54 0.46 ## [61,] 1.00 0.00 ## [62,] 0.00 1.00 ## [63,] 0.00 1.00 ## [64,] 0.00 1.00 ## [65,] 0.98 0.02 ## [66,] 0.00 1.00 ## [67,] 1.00 0.00 ## [68,] 0.07 0.93 ## [69,] 0.86 0.14 ## [70,] 0.99 0.01 ## [71,] 0.02 0.98 ## [72,] 0.00 1.00 ## [73,] 0.45 0.55 ## [74,] 0.96 0.04 ## [75,] 0.55 0.45 ## [76,] 0.19 0.81 ## [77,] 0.99 0.01 ## [78,] 0.05 0.95 ## [79,] 0.25 0.75 ## [80,] 0.30 0.70 ## [81,] 0.02 0.98 ## [82,] 0.00 1.00 fit_rowclust_cols$RowClusterMembers ## [[1]] ##  [1]  9 12 15 16 17 20 24 25 26 27 28 30 32 33 36 37 38 44 45 47 48 54 55 57 60 ## [26] 61 65 67 69 70 74 75 77 ##  ## [[2]] ##  [1]  1  2  3  4  5  6  7  8 10 11 13 14 18 19 21 22 23 29 31 34 35 39 40 41 42 ## [26] 43 46 49 50 51 52 53 56 58 59 62 63 64 66 68 71 72 73 76 78 79 80 81 82 fit_rowclust_cols$parlist.out$rowc ##     rowc_1     rowc_2  ## -0.8935557  0.8935557 boxplot(split(rowMeans(df), fit_rowclust_cols$RowClusters),     \"Mean response values across all questions for each individual\",     names = c(\"Cluster 1\", \"Cluster 2\")) round(fit_rowclust_cols$parlist.out$col,     2) ##    Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8    Q9   Q10   Q11   Q12  ##  6.80 -1.43 -2.79 -3.18 -0.62  0.84 -0.36  1.05  1.17  0.69 -1.72 -0.45"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"row-clusters-with-individual-column-effects-and-interactions","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering","what":"Row clusters with individual column effects and interactions","title":"`clustord` Tutorial","text":"third row clustering structure allows us include column effects, also interactions column effects rows. means datasets, individuals may exhibit particular pattern responses across questions varies clusters. example, cluster 1 individuals might tend answer initial questions high-value responses later questions low-value responses, whereas cluster 2 individuals might tend answer initial questions low-value responses later questions high-value responses. interaction pattern. model, linear predictor adds additional component, rowc_col, matrix parameters show interaction effects cluster column. IMPORTANT WARNING: Depending number columns dataset, especially 10, adding interaction term adds quite lot additional parameters model. total number non-dependent parameters : R−1R-1 (mixing proportion parameters indicate proportion rows cluster) + R−1R-1 (row cluster effects) + mm (number columns, mm) + (R−1)×(m−1)(R-1)\\times(m-1) (row-cluster column interactions) + q−1q-1 (category parameters {μk}\\{\\mu_k\\}, described ordinal models section). qq number categories/levels response variable. total number parameters approximately equal mRmR. Therefore may good idea fit model n<20mn < 20m. rigorously tested limit, rough rule thumb; least, fitting model carry model selection also carefully decide number iterations check convergence. case, dataset small, fit model illustrate process. interaction model also take longer fit previous two models, larger number parameters can lead longer convergence times. default, algorithm fits row-cluster-model model without interactions (called “intermediate rowcluster-column model” output) first order find good starting values parameters interaction model. often saves time reducing number iterations full model.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"fitting-the-model-2","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering > Row clusters with individual column effects and interactions","what":"Fitting the model","title":"`clustord` Tutorial","text":"formula interactions works regression models R, two options adding interactions. two formulae equivalent:","code":"Y ~ ROWCLUST + COL + ROWCLUST:COL ## Y ~ ROWCLUST + COL + ROWCLUST:COL Y ~ ROWCLUST * COL ## Y ~ ROWCLUST * COL set.seed(1) fit_rowclust_cols_interact <- clustord(Y ~     ROWCLUST * COL, \"POM\", nclus.row = 2,     long.df = long.df, verbose = FALSE)"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"checking-the-output-2","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering > Row clusters with individual column effects and interactions","what":"Checking the output","title":"`clustord` Tutorial","text":", fit completed first check converged: can look probabilities cluster membership: Let’s see lists cluster members: Now let’s look parameter values cluster: , can show first cluster higher response values second cluster, although now appears overlap:  Now let’s check column effect parameters: include interaction terms, column effects become bit larger columns. can finally check interaction effects: can plot interaction terms see interaction effects:  suggests individuals cluster 1 tend give low-value responses questions 1 6, high-value responses rest, reverse individuals cluster 2 (large cluster).","code":"fit_rowclust_cols_interact$EM.status$converged ## [1] TRUE round(fit_rowclust_cols_interact$ppr, 2) ##       [,1] [,2] ##  [1,] 0.89 0.11 ##  [2,] 1.00 0.00 ##  [3,] 1.00 0.00 ##  [4,] 1.00 0.00 ##  [5,] 1.00 0.00 ##  [6,] 1.00 0.00 ##  [7,] 1.00 0.00 ##  [8,] 0.96 0.04 ##  [9,] 0.24 0.76 ## [10,] 1.00 0.00 ## [11,] 1.00 0.00 ## [12,] 0.00 1.00 ## [13,] 1.00 0.00 ## [14,] 1.00 0.00 ## [15,] 0.00 1.00 ## [16,] 0.00 1.00 ## [17,] 0.00 1.00 ## [18,] 1.00 0.00 ## [19,] 1.00 0.00 ## [20,] 0.00 1.00 ## [21,] 0.40 0.60 ## [22,] 0.06 0.94 ## [23,] 1.00 0.00 ## [24,] 0.07 0.93 ## [25,] 0.00 1.00 ## [26,] 0.00 1.00 ## [27,] 0.01 0.99 ## [28,] 0.00 1.00 ## [29,] 1.00 0.00 ## [30,] 0.12 0.88 ## [31,] 1.00 0.00 ## [32,] 0.00 1.00 ## [33,] 0.00 1.00 ## [34,] 1.00 0.00 ## [35,] 1.00 0.00 ## [36,] 0.00 1.00 ## [37,] 0.00 1.00 ## [38,] 0.00 1.00 ## [39,] 0.99 0.01 ## [40,] 1.00 0.00 ## [41,] 1.00 0.00 ## [42,] 0.99 0.01 ## [43,] 1.00 0.00 ## [44,] 0.00 1.00 ## [45,] 0.23 0.77 ## [46,] 1.00 0.00 ## [47,] 0.29 0.71 ## [48,] 0.02 0.98 ## [49,] 0.99 0.01 ## [50,] 1.00 0.00 ## [51,] 1.00 0.00 ## [52,] 1.00 0.00 ## [53,] 1.00 0.00 ## [54,] 0.39 0.61 ## [55,] 0.05 0.95 ## [56,] 0.49 0.51 ## [57,] 0.08 0.92 ## [58,] 1.00 0.00 ## [59,] 1.00 0.00 ## [60,] 0.22 0.78 ## [61,] 0.00 1.00 ## [62,] 1.00 0.00 ## [63,] 1.00 0.00 ## [64,] 1.00 0.00 ## [65,] 0.00 1.00 ## [66,] 1.00 0.00 ## [67,] 0.00 1.00 ## [68,] 0.99 0.01 ## [69,] 0.03 0.97 ## [70,] 0.01 0.99 ## [71,] 1.00 0.00 ## [72,] 1.00 0.00 ## [73,] 0.08 0.92 ## [74,] 0.00 1.00 ## [75,] 0.99 0.01 ## [76,] 0.98 0.02 ## [77,] 0.00 1.00 ## [78,] 0.99 0.01 ## [79,] 0.94 0.06 ## [80,] 0.65 0.35 ## [81,] 0.94 0.06 ## [82,] 1.00 0.00 fit_rowclust_cols_interact$RowClusterMembers ## [[1]] ##  [1]  1  2  3  4  5  6  7  8 10 11 13 14 18 19 23 29 31 34 35 39 40 41 42 43 46 ## [26] 49 50 51 52 53 58 59 62 63 64 66 68 71 72 75 76 78 79 80 81 82 ##  ## [[2]] ##  [1]  9 12 15 16 17 20 21 22 24 25 26 27 28 30 32 33 36 37 38 44 45 47 48 54 55 ## [26] 56 57 60 61 65 67 69 70 73 74 77 fit_rowclust_cols_interact$parlist.out$rowc ##     rowc_1     rowc_2  ##  0.9715144 -0.9715144 boxplot(split(rowMeans(df), fit_rowclust_cols_interact$RowClusters),     \"Mean response values across all questions for each individual\",     names = c(\"Cluster 1\", \"Cluster 2\")) round(fit_rowclust_cols_interact$parlist.out$col,     2) ##    Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8    Q9   Q10   Q11   Q12  ##  7.14 -1.48 -2.91 -3.30 -0.72  1.10 -0.40  1.10  1.27  0.63 -1.84 -0.59 round(fit_rowclust_cols_interact$parlist.out$rowc_col,     2) ##         Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8    Q9   Q10   Q11   Q12 ## [1,] -1.18 -0.19 -0.12 -0.18  0.46 -0.94  0.29  0.33  0.14  0.64  0.11  0.63 ## [2,]  1.18  0.19  0.12  0.18 -0.46  0.94 -0.29 -0.33 -0.14 -0.64 -0.11 -0.63 rowc_col <- fit_rowclust_cols_interact$parlist.out$rowc_col plot(rowc_col[1, ], type = \"b\", col = \"black\",     lwd = 2, ylim = c(-1.3, 1.3)) lines(rowc_col[2, ], lty = 2, col = \"blue\",     lwd = 2) points(rowc_col[2, ], lty = 2, col = \"blue\",     lwd = 2) legend(\"bottomright\", legend = c(\"Cluster 1\",     \"Cluster 2\"), col = c(\"black\", \"blue\"),     lwd = c(2, 2), lty = 1:2)"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"model-selection","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering","what":"Model selection","title":"`clustord` Tutorial","text":"fitted multiple models, need select best one. advantage model-based clustering methods can use information criteria /likelihoods models select best one. Sticking two clusters, let’s see best three models . clustord() provides set information criteria listed , widely used AIC (Akaike, 1973) BIC (Schwarz, 1978), let’s use now. AIC BIC, lower values indicate better goodness--fit. can see according AIC third model, column effects interactions, best, whereas according BIC second model, column effects interactions, best. makes sense, BIC designed penalize complexity AIC , third model complex one. case, since values second third models roughly comparable, simplicity makes models easier interpret, choose stick second model rather third.","code":"fit_rowclust_only$criteria$AIC ## [1] 3854.975 fit_rowclust_cols$criteria$AIC ## [1] 2258.931 fit_rowclust_cols_interact$criteria$AIC ## [1] 2216.477 fit_rowclust_only$criteria$BIC ## [1] 3894.108 fit_rowclust_cols$criteria$BIC ## [1] 2351.872 fit_rowclust_cols_interact$criteria$BIC ## [1] 2363.225"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"incomplete-data-and-complete-data-log-likelihoods","dir":"Articles","previous_headings":"Fitting clustord models > Row clustering","what":"Incomplete-data and complete-data log-likelihoods","title":"`clustord` Tutorial","text":"Note information criteria provided clustord() based either incomplete-data complete-data log-likelihood. meaning “likelihood” statistical context probability obtaining observed data, given particular set parameter values. clustord(), like likelihood-based model-clustering methods, attempts find set parameter values highest likelihood, although can maximise parameters one single model time. fact, attempts maximise log-likelihood instead, numerically accurate unless really tiny dataset (< 20 rows columns) attempt fit mixture model requires maximisation parameters individual clusters (include rowc, col rowc_col parameters described ) “mixing proportions” clusters ({πr}\\{\\pi_r\\} parameters described ). clustord() uses EM algorithm perform model fitting, also treats cluster membership probabilities unknown quantities need estimated. clustord() therefore calculates two types likelihood. One complete-data log-likelihood, log-likelihood parameters mixing proportions, given specific set cluster membership probabilities (usually estimated probabilities latest EM algorithm iteration). second incomplete-data log-likelihood, log-likelihood parameters mixing proportions integrating possible cluster memberships. , context, called simply “log-likelihood” model. core log-likelihood need find, complete-data log-likelihood simply stepping-stone way finding . presence two types likelihood log-likelihood algorithm clustord() always labels every log-likelihood calculates either “complete-data” “incomplete-data”.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"column-clustering","dir":"Articles","previous_headings":"Fitting clustord models","what":"Column clustering","title":"`clustord` Tutorial","text":"far demonstrated cluster rows data matrix. can also cluster columns data matrix. example dataset, correspond clustering questions survey, find groups similar patterns responses. easily transposing data matrix running row clustering models , since column clustering row clustering models mathematically equivalent. transposition clustord() . apply column clustering models, clustord() transposes data, runs row clustering algorithm, flips data matrix output results back original orientation. Let’s try existing dataset. models structure .","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"column-cluster-effect-only","dir":"Articles","previous_headings":"Fitting clustord models > Column clustering","what":"Column cluster effect only","title":"`clustord` Tutorial","text":"simplest column clustering model column-cluster-model, main part linear predictor colc, column cluster effect. use case-sensitive keyword COLCLUST formula, need set nclus.column instead nclus.row: column clustering, mixing proportions renamed {κc}\\{\\kappa_c\\} (avoid confusion performing biclustering sets mixing proportions, seen ). cluster membership probabilities stored output ppc ppr cluster memberships named ColumnClusters RowClusters lists cluster members named ColumnClusterMembers RowClusterMembers. algorithm converged, columns firmly allocated one clusters, roughly equally split two clusters. parameter values indicate cluster 1 questions tend higher response values cluster 2 questions (note question highest typical responses, Q1, first cluster, ’d expect).","code":"set.seed(1) fit_colclust_only <- clustord(Y ~ COLCLUST,     model = \"POM\", nclus.column = 2, long.df = long.df,     verbose = FALSE) # Convergence fit_colclust_only$EM.status$converged ## [1] TRUE # Column cluster membership # probabilities round(fit_colclust_only$ppc, 2) ##       [,1] [,2] ##  [1,]    1    0 ##  [2,]    0    1 ##  [3,]    0    1 ##  [4,]    0    1 ##  [5,]    0    1 ##  [6,]    1    0 ##  [7,]    0    1 ##  [8,]    1    0 ##  [9,]    1    0 ## [10,]    1    0 ## [11,]    0    1 ## [12,]    0    1 # Members of each column cluster fit_colclust_only$ColumnClusterMembers ## [[1]] ## [1]  1  6  8  9 10 ##  ## [[2]] ## [1]  2  3  4  5  7 11 12 # Mixing proportions round(fit_colclust_only$kappa.out, 2) ## [1] 0.42 0.58 # Parameters fit_colclust_only$parlist.out$colc ##    colc_1    colc_2  ##  1.201864 -1.201864"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"column-clusters-with-individual-row-effects","dir":"Articles","previous_headings":"Fitting clustord models > Column clustering","what":"Column clusters with individual row effects","title":"`clustord` Tutorial","text":"Similarly row clustering, column clustering can fit model incorporates individual row effects, account fact individual subjects may different response patterns others. uses case-sensitive keyword ROW. see case, column clustering individual row effects detected column Q1 different columns, cluster , much higher probability getting high value responses. almost zero uncertainty cluster memberships. rows big negative parameters, thus much likely show lower-value responses. probably identified row clustering process lower response values rest.","code":"set.seed(1) fit_colclust_rows <- clustord(Y ~ COLCLUST +     ROW, model = \"POM\", nclus.column = 2,     long.df = long.df, verbose = FALSE) # Convergence fit_colclust_rows$EM.status$converged ## [1] TRUE # Column cluster membership # probabilities round(fit_colclust_rows$ppc, 2) ##       [,1] [,2] ##  [1,]    1    0 ##  [2,]    0    1 ##  [3,]    0    1 ##  [4,]    0    1 ##  [5,]    0    1 ##  [6,]    0    1 ##  [7,]    0    1 ##  [8,]    0    1 ##  [9,]    0    1 ## [10,]    0    1 ## [11,]    0    1 ## [12,]    0    1 # Members of each column cluster fit_colclust_rows$ColumnClusterMembers ## [[1]] ## [1] 1 ##  ## [[2]] ##  [1]  2  3  4  5  6  7  8  9 10 11 12 # Mixing proportions round(fit_colclust_rows$kappa.out, 2) ## [1] 0.08 0.92 # Parameters fit_colclust_rows$parlist.out$colc ##    colc_1    colc_2  ##  3.493738 -3.493738 round(fit_colclust_rows$parlist.out$row,     2) ##   ID1   ID2   ID3   ID4   ID5   ID6   ID7   ID8   ID9  ID10  ID11  ID12  ID13  ## -0.01  0.62  0.65  1.07  1.08  0.40  1.87  0.08 -0.20  0.92  0.28 -1.33  1.74  ##  ID14  ID15  ID16  ID17  ID18  ID19  ID20  ID21  ID22  ID23  ID24  ID25  ID26  ##  1.02 -0.74 -0.71 -2.73  0.30  0.50 -0.47  0.01 -0.18  0.49 -0.52 -0.45 -0.71  ##  ID27  ID28  ID29  ID30  ID31  ID32  ID33  ID34  ID35  ID36  ID37  ID38  ID39  ## -0.72 -0.48  0.26 -0.60  0.55 -0.92 -3.00  0.57  0.97 -1.05 -0.88 -1.86  0.50  ##  ID40  ID41  ID42  ID43  ID44  ID45  ID46  ID47  ID48  ID49  ID50  ID51  ID52  ##  0.80  0.51  0.08  0.13 -0.69 -0.39  0.57 -0.74 -0.53  0.23  1.25  0.71  1.11  ##  ID53  ID54  ID55  ID56  ID57  ID58  ID59  ID60  ID61  ID62  ID63  ID64  ID65  ##  0.65  0.00 -0.64  0.01 -0.53  0.65  0.42 -0.10 -3.71  2.96  2.27  0.75 -0.71  ##  ID66  ID67  ID68  ID69  ID70  ID71  ID72  ID73  ID74  ID75  ID76  ID77  ID78  ##  1.20 -4.27  0.17 -0.30 -0.73  0.27  1.48 -0.18 -0.48 -0.15  0.05 -0.84  0.18  ##  ID79  ID80  ID81  ID82  ##  0.00  0.05  0.55  1.63"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"column-clusters-with-individual-row-effects-and-interaction","dir":"Articles","previous_headings":"Fitting clustord models > Column clustering","what":"Column clusters with individual row effects and interaction","title":"`clustord` Tutorial","text":", , can add interactions column cluster effects individual row effects, using usual R interaction formula notation. interaction terms added named colc_row. Adding interactions individual rows column clusters changed cluster memberships, compared model without interactions, made effects clusters stronger, seen colc parameters, made individual row effects much bigger. can plot interaction terms see interaction effects:  can see huge difference response values cluster 1 (Q1) cluster 2 (rest questions) last subject, cluster responses broadly similar subjects. Zooming rest plot:  clear pattern , often suggests model well-fitted.","code":"set.seed(1) fit_colclust_rows_interact <- clustord(Y ~     COLCLUST * ROW, model = \"POM\", nclus.column = 2,     long.df = long.df, verbose = FALSE) # Convergence fit_colclust_rows_interact$EM.status$converged ## [1] TRUE # Column cluster membership # probabilities round(fit_colclust_rows_interact$ppc, 2) ##       [,1] [,2] ##  [1,]    1    0 ##  [2,]    0    1 ##  [3,]    0    1 ##  [4,]    0    1 ##  [5,]    0    1 ##  [6,]    0    1 ##  [7,]    0    1 ##  [8,]    0    1 ##  [9,]    0    1 ## [10,]    0    1 ## [11,]    0    1 ## [12,]    0    1 # Members of each column cluster fit_colclust_rows_interact$ColumnClusterMembers ## [[1]] ## [1] 1 ##  ## [[2]] ##  [1]  2  3  4  5  6  7  8  9 10 11 12 # Mixing proportions round(fit_colclust_rows_interact$kappa.out,     2) ## [1] 0.08 0.92 # Parameters fit_colclust_rows_interact$parlist.out$colc ##    colc_1    colc_2  ##  10.49554 -10.49554 round(fit_colclust_rows_interact$parlist.out$row,     2) ##    ID1    ID2    ID3    ID4    ID5    ID6    ID7    ID8    ID9   ID10   ID11  ##  -3.89   3.58   3.63  -3.61   3.87  -3.76   4.30  -3.86  -3.92   3.78  -7.83  ##   ID12   ID13   ID14   ID15   ID16   ID17   ID18   ID19   ID20   ID21   ID22  ##   2.64   4.23   3.83  -4.06   2.93   1.79  -3.83   3.54   3.03   3.28   3.18  ##   ID23   ID24   ID25   ID26   ID27   ID28   ID29   ID30   ID31   ID32   ID33  ##  -7.73  -8.26   3.00   2.92  -4.05   3.02  -3.83  -4.00   3.57   2.83  -5.90  ##   ID34   ID35   ID36   ID37   ID38   ID39   ID40   ID41   ID42   ID43   ID44  ##  -3.77   3.81  -4.19  -8.45  -4.76   3.54   3.72  -3.78  -7.94  -8.60   2.90  ##   ID45   ID46   ID47   ID48   ID49   ID50   ID51   ID52   ID53   ID54   ID55  ##  -3.96  -7.67  -8.35  -3.99  -3.80   3.97   3.66   3.88  -3.71   3.25  -4.02  ##   ID56   ID57   ID58   ID59   ID60   ID61   ID62   ID63   ID64   ID65   ID66  ##   3.28  -3.99   3.63  -3.79   3.20 -10.20   4.85   4.49  -7.53   2.92  -7.27  ##   ID67   ID68   ID69   ID70   ID71   ID72   ID73   ID74   ID75   ID76   ID77  ##  -2.78  -3.85   3.12  -4.06  -7.79  -7.17   3.18   3.02  -8.72   3.25   2.86  ##   ID78   ID79   ID80   ID81   ID82  ##  -3.86  -3.87   3.29   3.57  84.06 round(fit_colclust_rows_interact$parlist.out$colc_row,     2) ##        ID1   ID2   ID3 ID4   ID5   ID6   ID7   ID8   ID9  ID10  ID11  ID12 ID13 ## [1,] -4.09  3.01  2.96  -5  2.75 -4.51  2.33 -4.19 -3.91  2.83 -8.46  4.14  2.4 ## [2,]  4.09 -3.01 -2.96   5 -2.75  4.51 -2.33  4.19  3.91 -2.83  8.46 -4.14 -2.4 ##       ID14  ID15  ID16  ID17  ID18  ID19 ID20 ID21  ID22  ID23  ID24  ID25 ## [1,]  2.78 -3.44  3.75  4.98 -4.38  3.04  3.6  3.3  3.41 -8.56 -8.03  3.63 ## [2,] -2.78  3.44 -3.75 -4.98  4.38 -3.04 -3.6 -3.3 -3.41  8.56  8.03 -3.63 ##       ID26  ID27  ID28  ID29  ID30  ID31  ID32  ID33  ID34 ID35  ID36  ID37 ## [1,]  3.75 -3.46  3.61 -4.35 -3.56  3.01  3.88 -2.57 -4.61  2.8 -3.21 -7.84 ## [2,] -3.75  3.46 -3.61  4.35  3.56 -3.01 -3.88  2.57  4.61 -2.8  3.21  7.84 ##       ID38  ID39  ID40  ID41  ID42 ID43  ID44  ID45  ID46  ID47  ID48  ID49 ## [1,] -2.85  3.04  2.88 -4.56 -8.35 -9.1  3.77 -3.74 -8.62 -7.94 -3.62 -4.36 ## [2,]  2.85 -3.04 -2.88  4.56  8.35  9.1 -3.77  3.74  8.62  7.94  3.62  4.36 ##       ID50  ID51  ID52  ID53  ID54  ID55 ID56  ID57  ID58 ID59  ID60  ID61 ## [1,]  2.66  2.94  2.75 -4.72  3.32 -3.53  3.3 -3.62  2.96 -4.5  3.39 -6.09 ## [2,] -2.66 -2.94 -2.75  4.72 -3.32  3.53 -3.3  3.62 -2.96  4.5 -3.39  6.09 ##       ID62  ID63  ID64  ID65  ID66  ID67  ID68  ID69  ID70 ID71  ID72  ID73 ## [1,]  1.73  2.11 -8.76  3.75 -9.02  9.36 -4.27  3.49 -3.44 -8.5 -9.12  3.41 ## [2,] -1.73 -2.11  8.76 -3.75  9.02 -9.36  4.27 -3.49  3.44  8.5  9.12 -3.41 ##       ID74  ID75  ID76  ID77  ID78  ID79  ID80  ID81   ID82 ## [1,]  3.61 -8.98  3.32  3.84 -4.27 -4.11  3.29  3.01  82.36 ## [2,] -3.61  8.98 -3.32 -3.84  4.27  4.11 -3.29 -3.01 -82.36 colc_row <- fit_colclust_rows_interact$parlist.out$colc_row plot(colc_row[1, ], type = \"b\", col = \"black\",     lwd = 2, ylim = c(-85, 85), xlab = \"Subject\",     ylab = \"Cluster interaction effect\") lines(colc_row[2, ], lty = 2, col = \"blue\",     lwd = 2) points(colc_row[2, ], lty = 3, col = \"blue\",     lwd = 2) legend(\"topleft\", legend = c(\"Cluster 1\",     \"Cluster 2\"), col = c(\"black\", \"blue\"),     lwd = c(2, 2), lty = c(1, 3)) colc_row <- fit_colclust_rows_interact$parlist.out$colc_row plot(colc_row[1, ], type = \"b\", col = \"black\",     lwd = 2, ylim = c(-10, 10), xlab = \"Subject\",     ylab = \"Cluster interaction effect\") lines(colc_row[2, ], lty = 2, col = \"blue\",     lwd = 2) points(colc_row[2, ], lty = 3, col = \"blue\",     lwd = 2) legend(\"topleft\", legend = c(\"Cluster 1\",     \"Cluster 2\"), col = c(\"black\", \"blue\"),     lwd = c(2, 2), lty = c(1, 3))"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"model-selection-1","dir":"Articles","previous_headings":"Fitting clustord models > Column clustering","what":"Model selection","title":"`clustord` Tutorial","text":", look AIC BIC perform model selection amongst column clustering models. instance, AIC selects model individual row effects interactions, though similar AIC model individual row effects interactions. BIC selects model column clusters . AIC BIC disagree model select, time include external factors, important simplicity sake helping end-users understand model. Even AIC, value column-cluster-model great deal higher AIC values two models, justifiable select column-cluster-model simplicity match BIC selection.","code":"fit_colclust_only$criteria$AIC ## [1] 2792.468 fit_colclust_rows$criteria$AIC ## [1] 2572.39 fit_colclust_rows_interact$criteria$AIC ## [1] 2558.218 fit_colclust_only$criteria$BIC ## [1] 2831.601 fit_colclust_rows$criteria$BIC ## [1] 3007.744 fit_colclust_rows_interact$criteria$BIC ## [1] 3389.795"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"biclustering","dir":"Articles","previous_headings":"Fitting clustord models","what":"Biclustering","title":"`clustord` Tutorial","text":"far demonstrated cluster rows columns data matrix. clustord can also cluster rows columns simultaneously. call biclustering “two-mode clustering”, though term universally used (Jacques Biernacki (2018) example, calls “co-clustering”). example dataset, correspond clustering subjects questions survey, find subsets subjects similar patterns responses particular subsets questions. two biclustering structures: row column effects, row column effects interactions . biclustering form clustord allow inclusion individual row column effects addition row column clusters, makes model complex number parameters usually high fit well. ways model form complex row column clustering, can use fewer parameters row column clustering individual column row effects. considering types models use, think like : think primary goal row clustering lot variety amongst columns, row clustering individual column effects may suitable model; lot similar columns biclustering may suitable. Attempt fit types model, use model selection via e.g. AIC BIC find best model. Even main focus clustering columns, example, see variety amongst rows, good try fitting biclustering model account variety allow get accurate fit column clusters.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"biclustering-without-interactions","dir":"Articles","previous_headings":"Fitting clustord models > Biclustering","what":"Biclustering without interactions","title":"`clustord` Tutorial","text":"simpler biclustering model one row column cluster effects, without interactions . model rowc + colc main part linear predictor. define nclus.row nclus.column. default, biclustering model fits row clustering column clustering models first order find good starting points parameters biclustering model, row column clustering models quicker run. Note reporting biclustering model also reports complete-data log-likelihood APPROXIMATE incomplete-data log-likelihood. true incomplete-data log-likelihood infeasible calculate, even two row clusters two column clusters, use entropy-based approximation calculate . biclustering, obtain cluster membership proportions row column clusters (ppr ppc), mixing proportions ({πr}\\{\\pi_r\\} {κc}\\{\\kappa_c\\}). maximum-probability cluster memberships named RowClusters ColumnClusters lists cluster memberships RowClusterMembers ColumnClusterMembers. cluster probabilities close 1 0 row column clusters. row clusters identified small big ones row clustering, although column clusters now little evenly split clusters. see first row cluster higher response values, first column cluster higher response values.","code":"set.seed(4) fit_biclust <- clustord(Y ~ ROWCLUST + COLCLUST,     model = \"POM\", nclus.row = 2, nclus.column = 2,     long.df = long.df, verbose = FALSE) converged <- fit_biclust$EM.status$converged # Convergence fit_biclust$EM.status$converged ## [1] FALSE # Cluster membership probabilities round(fit_biclust$ppr, 2) ##       [,1] [,2] ##  [1,] 0.99 0.01 ##  [2,] 0.97 0.03 ##  [3,] 0.98 0.02 ##  [4,] 1.00 0.00 ##  [5,] 1.00 0.00 ##  [6,] 0.99 0.01 ##  [7,] 1.00 0.00 ##  [8,] 0.99 0.01 ##  [9,] 0.97 0.03 ## [10,] 1.00 0.00 ## [11,] 1.00 0.00 ## [12,] 0.01 0.99 ## [13,] 1.00 0.00 ## [14,] 0.99 0.01 ## [15,] 0.79 0.21 ## [16,] 0.15 0.85 ## [17,] 0.00 1.00 ## [18,] 1.00 0.00 ## [19,] 0.95 0.05 ## [20,] 0.31 0.69 ## [21,] 0.86 0.14 ## [22,] 0.76 0.24 ## [23,] 1.00 0.00 ## [24,] 0.93 0.07 ## [25,] 0.39 0.61 ## [26,] 0.11 0.89 ## [27,] 0.58 0.42 ## [28,] 0.21 0.79 ## [29,] 1.00 0.00 ## [30,] 0.75 0.25 ## [31,] 0.98 0.02 ## [32,] 0.06 0.94 ## [33,] 0.00 1.00 ## [34,] 1.00 0.00 ## [35,] 1.00 0.00 ## [36,] 0.50 0.50 ## [37,] 0.70 0.30 ## [38,] 0.01 0.99 ## [39,] 0.95 0.05 ## [40,] 0.99 0.01 ## [41,] 1.00 0.00 ## [42,] 0.99 0.01 ## [43,] 0.99 0.01 ## [44,] 0.06 0.94 ## [45,] 0.94 0.06 ## [46,] 1.00 0.00 ## [47,] 0.88 0.12 ## [48,] 0.80 0.20 ## [49,] 0.99 0.01 ## [50,] 1.00 0.00 ## [51,] 0.97 0.03 ## [52,] 0.99 0.01 ## [53,] 1.00 0.00 ## [54,] 0.53 0.47 ## [55,] 0.74 0.26 ## [56,] 0.86 0.14 ## [57,] 0.80 0.20 ## [58,] 0.98 0.02 ## [59,] 0.99 0.01 ## [60,] 0.59 0.41 ## [61,] 0.00 1.00 ## [62,] 1.00 0.00 ## [63,] 1.00 0.00 ## [64,] 1.00 0.00 ## [65,] 0.11 0.89 ## [66,] 1.00 0.00 ## [67,] 0.00 1.00 ## [68,] 0.99 0.01 ## [69,] 0.43 0.57 ## [70,] 0.73 0.27 ## [71,] 0.99 0.01 ## [72,] 1.00 0.00 ## [73,] 0.76 0.24 ## [74,] 0.21 0.79 ## [75,] 0.96 0.04 ## [76,] 0.83 0.17 ## [77,] 0.12 0.88 ## [78,] 0.99 0.01 ## [79,] 0.99 0.01 ## [80,] 0.76 0.24 ## [81,] 0.98 0.02 ## [82,] 1.00 0.00 round(fit_biclust$ppc, 2) ##       [,1] [,2] ##  [1,]    0    1 ##  [2,]    1    0 ##  [3,]    1    0 ##  [4,]    1    0 ##  [5,]    1    0 ##  [6,]    0    1 ##  [7,]    1    0 ##  [8,]    0    1 ##  [9,]    0    1 ## [10,]    0    1 ## [11,]    1    0 ## [12,]    1    0 # Members of each cluster fit_biclust$RowClusterMembers ## [[1]] ##  [1]  1  2  3  4  5  6  7  8  9 10 11 13 14 15 18 19 21 22 23 24 27 29 30 31 34 ## [26] 35 37 39 40 41 42 43 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 62 63 ## [51] 64 66 68 70 71 72 73 75 76 78 79 80 81 82 ##  ## [[2]] ##  [1] 12 16 17 20 25 26 28 32 33 36 38 44 61 65 67 69 74 77 fit_biclust$ColumnClusterMembers ## [[1]] ## [1]  2  3  4  5  7 11 12 ##  ## [[2]] ## [1]  1  6  8  9 10 # Mixing proportions round(fit_biclust$pi.out, 2) ## [1] 0.75 0.25 round(fit_biclust$kappa.out, 2) ## [1] 0.58 0.42 # Parameters fit_biclust$parlist.out$rowc ##     rowc_1     rowc_2  ##  0.8698192 -0.8698192 fit_biclust$parlist.out$colc ##    colc_1    colc_2  ## -1.183548  1.183548"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"biclustering-with-interactions","dir":"Articles","previous_headings":"Fitting clustord models > Biclustering","what":"Biclustering with interactions","title":"`clustord` Tutorial","text":"final structure biclustering model row column clusters interactions . introduces final element linear predictor, rowc_colc. models without interactions detected similar clustering structures. reassuring, indicates particular clustering structure fairly robust sensitive specific choice model.","code":"set.seed(3) fit_biclust_interact <- clustord(Y ~ ROWCLUST *     COLCLUST, model = \"POM\", nclus.row = 2,     nclus.column = 2, long.df = long.df,     verbose = FALSE) converged <- fit_biclust_interact$EM.status$converged # Convergence fit_biclust_interact$EM.status$converged ## [1] TRUE # Cluster membership probabilities round(fit_biclust_interact$ppr, 2) ##       [,1] [,2] ##  [1,] 1.00 0.00 ##  [2,] 1.00 0.00 ##  [3,] 1.00 0.00 ##  [4,] 1.00 0.00 ##  [5,] 1.00 0.00 ##  [6,] 1.00 0.00 ##  [7,] 1.00 0.00 ##  [8,] 1.00 0.00 ##  [9,] 1.00 0.00 ## [10,] 1.00 0.00 ## [11,] 1.00 0.00 ## [12,] 0.99 0.01 ## [13,] 1.00 0.00 ## [14,] 1.00 0.00 ## [15,] 1.00 0.00 ## [16,] 1.00 0.00 ## [17,] 0.01 0.99 ## [18,] 1.00 0.00 ## [19,] 1.00 0.00 ## [20,] 1.00 0.00 ## [21,] 1.00 0.00 ## [22,] 1.00 0.00 ## [23,] 1.00 0.00 ## [24,] 1.00 0.00 ## [25,] 1.00 0.00 ## [26,] 1.00 0.00 ## [27,] 1.00 0.00 ## [28,] 1.00 0.00 ## [29,] 1.00 0.00 ## [30,] 1.00 0.00 ## [31,] 1.00 0.00 ## [32,] 1.00 0.00 ## [33,] 0.00 1.00 ## [34,] 1.00 0.00 ## [35,] 1.00 0.00 ## [36,] 1.00 0.00 ## [37,] 1.00 0.00 ## [38,] 0.88 0.12 ## [39,] 1.00 0.00 ## [40,] 1.00 0.00 ## [41,] 1.00 0.00 ## [42,] 1.00 0.00 ## [43,] 1.00 0.00 ## [44,] 1.00 0.00 ## [45,] 1.00 0.00 ## [46,] 1.00 0.00 ## [47,] 1.00 0.00 ## [48,] 1.00 0.00 ## [49,] 1.00 0.00 ## [50,] 1.00 0.00 ## [51,] 1.00 0.00 ## [52,] 1.00 0.00 ## [53,] 1.00 0.00 ## [54,] 1.00 0.00 ## [55,] 1.00 0.00 ## [56,] 1.00 0.00 ## [57,] 1.00 0.00 ## [58,] 1.00 0.00 ## [59,] 1.00 0.00 ## [60,] 1.00 0.00 ## [61,] 0.00 1.00 ## [62,] 1.00 0.00 ## [63,] 1.00 0.00 ## [64,] 1.00 0.00 ## [65,] 1.00 0.00 ## [66,] 1.00 0.00 ## [67,] 0.00 1.00 ## [68,] 1.00 0.00 ## [69,] 1.00 0.00 ## [70,] 1.00 0.00 ## [71,] 1.00 0.00 ## [72,] 1.00 0.00 ## [73,] 1.00 0.00 ## [74,] 1.00 0.00 ## [75,] 1.00 0.00 ## [76,] 1.00 0.00 ## [77,] 1.00 0.00 ## [78,] 1.00 0.00 ## [79,] 1.00 0.00 ## [80,] 1.00 0.00 ## [81,] 1.00 0.00 ## [82,] 1.00 0.00 round(fit_biclust_interact$ppc, 2) ##       [,1] [,2] ##  [1,]    1    0 ##  [2,]    0    1 ##  [3,]    0    1 ##  [4,]    0    1 ##  [5,]    1    0 ##  [6,]    1    0 ##  [7,]    1    0 ##  [8,]    1    0 ##  [9,]    1    0 ## [10,]    1    0 ## [11,]    0    1 ## [12,]    1    0 # Members of each cluster fit_biclust_interact$RowClusterMembers ## [[1]] ##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24 25 26 ## [26] 27 28 29 30 31 32 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 ## [51] 53 54 55 56 57 58 59 60 62 63 64 65 66 68 69 70 71 72 73 74 75 76 77 78 79 ## [76] 80 81 82 ##  ## [[2]] ## [1] 17 33 61 67 fit_biclust_interact$ColumnClusterMembers ## [[1]] ## [1]  1  5  6  7  8  9 10 12 ##  ## [[2]] ## [1]  2  3  4 11 # Mixing proportions round(fit_biclust_interact$pi.out, 2) ## [1] 0.95 0.05 round(fit_biclust_interact$kappa.out, 2) ## [1] 0.67 0.33 # Parameters fit_biclust_interact$parlist.out$rowc ##    rowc_1    rowc_2  ##  3.001568 -3.001568 fit_biclust_interact$parlist.out$colc ##    colc_1    colc_2  ##  2.328658 -2.328658 round(fit_biclust_interact$parlist.out$rowc_colc,     2) ##       [,1]  [,2] ## [1,] -1.07  1.07 ## [2,]  1.07 -1.07"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"model-selection-2","dir":"Articles","previous_headings":"Fitting clustord models > Biclustering","what":"Model selection","title":"`clustord` Tutorial","text":"can use model selection assess two biclustering models best, can also use select whether biclustering models better row clustering model column clustering model. Let’s assess row clustering vs. biclustering comparison. AIC selects row clustering model individual column effects interactions best, row clustering without interactions close second. BIC selects row clustering model individual column effects interactions best row clustering model interactions close second. appears case, row clustering model better biclustering model, note biclustering models still better row-cluster-model large margin. Now let’s compare column clustering biclustering. nuanced picture. AIC roughly similar five models, best model individual row effects interactions. BIC worst model similar model without interactions. reason column clustering individual row effects, 82 rows dataset 82 parameters individual row effects huge number extra parameters include interactions. BIC naturally penalizes models lot. contrast, look row clustering models 12 columns, adding individual column effects add huge number parameters model allow flexibility model. biclustering model slightly lower AIC BIC column-cluster-model. Overall, main focus finding row clusters choose row clustering model individual column effects, whether include interactions, main focus finding column clusters choose biclustering model, without interaction, allows us incorporate bit variety amongst rows without adding much complexity model.","code":"fit_rowclust_only$criteria$AIC ## [1] 3854.975 fit_rowclust_cols$criteria$AIC ## [1] 2258.931 fit_rowclust_cols_interact$criteria$AIC ## [1] 2216.477 fit_biclust$criteria$AIC ## [1] 2951.194 fit_biclust_interact$criteria$AIC ## [1] 2692.541 fit_rowclust_only$criteria$BIC ## [1] 3894.108 fit_rowclust_cols$criteria$BIC ## [1] 2351.872 fit_rowclust_cols_interact$criteria$BIC ## [1] 2363.225 fit_biclust$criteria$BIC ## [1] 3000.11 fit_biclust_interact$criteria$BIC ## [1] 2746.349 fit_colclust_only$criteria$AIC ## [1] 2792.468 fit_colclust_rows$criteria$AIC ## [1] 2572.39 fit_colclust_rows_interact$criteria$AIC ## [1] 2558.218 fit_biclust$criteria$AIC ## [1] 2951.194 fit_biclust_interact$criteria$AIC ## [1] 2692.541 fit_colclust_only$criteria$BIC ## [1] 2831.601 fit_colclust_rows$criteria$BIC ## [1] 3007.744 fit_colclust_rows_interact$criteria$BIC ## [1] 3389.795 fit_biclust$criteria$BIC ## [1] 3000.11 fit_biclust_interact$criteria$BIC ## [1] 2746.349"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"number-of-clusters","dir":"Articles","previous_headings":"","what":"Number of clusters","title":"`clustord` Tutorial","text":"models used 2 row clusters /2 column clusters, simplicity. course always suitable number clusters. clustord offer method automatically fitting multiple different numbers clusters. want try different numbers clusters, run different fits different numbers clusters. can compare using model selection procedure shown . think likely need 3 clusters, example, advisable also try 4 5 clusters, rather just 4. reason might find AIC slightly lower 3 clusters 4, AIC might drop lower 5, .e. minimum AIC 3 clusters might just local minimum rather global minimum, searching bit widely number clusters can avoid trap. also possible try different numbers clusters different models want try, compare results. find every model best result 3 rather 4 clusters, fairly strong indication 3 best number clusters, whereas one structure selects 3 clusters best another structure selects 4 clusters best, clear answer “best” number clusters may need consider external factors judging many clusters use.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"important-algorithm-settings","dir":"Articles","previous_headings":"","what":"Important algorithm settings","title":"`clustord` Tutorial","text":"clustord() algorithm complex, many settings, handful particularly important understand achieving good clustering results. key parameters EMcycles startEMcycles inside EM.control argument, nstarts argument. related. EM algorithm works iteratively improving parameter estimates estimated cluster memberships. start estimates, known sometimes sensitive initial estimates. can, therefore, get stuck near set parameter estimates better similar values, best.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"nstarts","dir":"Articles","previous_headings":"Important algorithm settings","what":"nstarts","title":"`clustord` Tutorial","text":"One simple way algorithm gets around test multiple different starting points, choose best one. nstarts controls many different starting points algorithm tries. general, complex model, starts try, parameters greater chance algorithm getting stuck somewhere unhelpful. default number starting points 5. 2 3 clusters, ’re fitting cluster-models, probably enough. using model individual row/column effects lot individual rows columns, especially fitting interaction terms, good idea increase number random starts 10 20.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"emcycles-and-startemcycles","dir":"Articles","previous_headings":"Important algorithm settings","what":"EMcycles and startEMcycles","title":"`clustord` Tutorial","text":"EMcycles, one entries EM.control argument, maximum number EM iterations. examples , checked time whether EM algorithm converged looking rest output. algorithm converged, try running random starts, running use different random starts, ’ve used random seed rerun different seed. ’ve already tried quite different random starts /different random seeds still can’t get converge, try increasing number iterations, lack convergence means hit upper limit number iterations reached convergence. default number EMcycles 50, try 100, example. startEMcycles another setting EM.control argument, controls number EM iterations algorithm goes random start. 5 default, high. takes EM algorithm reach convergence, takes iterations algorithm distinguish different starting points. differences starting points usually much bigger improvement can achieved iterations. default number startEMcycles 5, using lots random starts, e.g. least 20, may want change value 2 3, example, save bit computing time. want set EMcycles startEMcycles, input part EM.control argument, list object. EM.control list settings default, set don’t want ; can simply set ones want. works way control argument R works. , example, can run clustord() additional random starts fewer starting iterations main iterations defaults: rest settings discussed Advanced Settings vignette.","code":"fit <- clustord(Y ~ ROWCLUST + COL, model = \"POM\",     nclus.row = 2, long.df = long.df, EM.control = list(startEMcycles = 2,         EMcycles = 100), nstarts = 10)"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"clustering-with-covariates","dir":"Articles","previous_headings":"","what":"Clustering with covariates","title":"`clustord` Tutorial","text":"big advantage using clustord package clustering models uses can include covariates. Just row column cluster effects can change responses data matrix, covariates can also effects responses. example, data matrix set responses survey questions, also additional demographic information individuals think might affected answered questions, can include demographics covariates rows. priori information people likely answer particular question survey, can include covariate columns.  example shows row covariates, .e. additions model indexed ii. case, covariate numerical covariate gives age survey respondent.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"data-format-for-covariates","dir":"Articles","previous_headings":"Clustering with covariates","what":"Data format for covariates","title":"`clustord` Tutorial","text":"want use covariates, added long form data frame used clustering. can feed mat2df() function along original data matrix, function automatically add long form data frame. example simulates age covariate survey dataset includes long form data frame. adding covariates rows data matrix, .e. covariates take different values different rows, need supply using xr.df argument mat2df(). can add many covariates like, numerical categorical, just setting data frame regression analysis. mat2df() function handle converting categorical covariates dummy variables, just lm() glm() . instead adding covariates columns data matrix, .e. covariates take different values different columns, need supply using xc.df argument mat2df(), although can add many numerical categorical covariates like. example simulates covariate columns survey dataset, adds age covariate long form data frame.","code":"age.df <- data.frame(age = round(runif(nrow(df),     min = 20, max = 60)))  long.df <- mat2df(df, xr.df = age.df) ## Warning in mat2df(df, xr.df = age.df): Removing 4 entries for which Y is NA. question.df <- data.frame(question = sample(c(\"Group A\",     \"Group B\"), ncol(df), replace = TRUE))  long.df <- mat2df(df, xr.df = age.df, xc.df = question.df) ## Warning in mat2df(df, xr.df = age.df, xc.df = question.df): Removing 4 entries ## for which Y is NA."},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"fitting-a-model-with-covariates","dir":"Articles","previous_headings":"Clustering with covariates","what":"Fitting a model with covariates","title":"`clustord` Tutorial","text":"Adding covariates model just like adding covariates regression model: include formula, can also add interactions clusters functions covariates logs powers. example performs row clustering addition age question covariates. covariate names must match names original data frames. Within parameter output object {μk}\\{\\mu_k\\} parameters discussed ordinal models section, row cluster effects rowc. cov parameters effects two covariates. first covariate, age, numerical covariate corresponds effect 1 unit increase covariate value. second covariate, question, categorical first level alphabetically, “Group ”, reference level coefficient shows effect Group B instead Group . Note whilst can include interactions covariates ROWCLUST COLCLUST formula, quite interactions covariates. formula xr row covariate , main part linear predictor, means term linear predictor involves row covariate xr (index row covariate), cluster (indexed r) different coefficient covariate (distinct non-interaction covariate models , coefficients covariates regardless cluster row ). slightly different interaction terms involving covariates, two covariates appear multiplied together model shared coefficient term. example shows formula main part linear predictor model row column covariate interacting: coefficient interaction term. ~~~ Y ~ ROWCLUST + xrxc rowc_coef_r + cov_coef1xr_i + cov_coef2xc_j + cov_coef1xr_i*xc_j ~~~ Also note can include first-level interactions ROWCLUST COLCLUST, fit third-level interactions covariate ROWCLUST COLCLUST. , terms like x:ROWCLUST:COLCLUST permitted clustord, due number parameters need fitted.","code":"fit_with_covariates <- clustord(Y ~ ROWCLUST +     age + question, model = \"POM\", nclus.row = 2,     long.df = long.df, verbose = FALSE) fit_with_covariates$EM.status$converged ## [1] FALSE fit_with_covariates$RowClusterMembers ## [[1]] ##  [1]  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 18 19 20 21 22 23 24 25 26 27 ## [26] 28 29 30 31 32 34 35 36 37 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [51] 55 56 57 58 59 60 62 63 64 65 66 68 69 70 71 72 73 74 75 76 77 78 79 80 81 ## [76] 82 ##  ## [[2]] ## [1] 12 17 33 38 61 67 fit_with_covariates$parlist.out ## $mu ##       mu_1       mu_2       mu_3       mu_4       mu_5       mu_6  ## -0.9972361  1.0423167  2.8071614  3.6728607  4.1871719 26.5198712  ##  ## $rowc ##    rowc_1    rowc_2  ##  1.310333 -1.310333  ##  ## $cov ##        cov_l        cov_l  ##  0.001709197 -0.956487126 Y ~ ROWCLUST*xr rowc_coef_r + rowc_row_coef_r1*xr_i + cov_coef*xr_i"},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"label-switching","dir":"Articles","previous_headings":"","what":"Label switching","title":"`clustord` Tutorial","text":"clusters clustord output labelled 1, 2, 3, etc. numbers meaningless. result clusters labelled 1, 2, 3, 4 mathematically equivalent results clusters numbered 2, 1, 4, 3 3, 1, 2, 4, etc. things genuinely distinguish one cluster another cluster parameters estimated probabilities membership clusters. Due random starting points, two runs clustord parameters can produce different results, unless fix random number seed first using set.seed(). run clustord() e.g. three row clusters, find rowc parameters (.e. main clustering effects) (-1.2, 1.4 0.2) run find parameters now (-0.3, -1.1, 1.4), sets parameter values roughly , just different order. change cluster order meaningless – simply “label switching” action. Similarly, one run 3 clusters another run 4 clusters, one parameters (1.4, 0.1 -1.5) parameters (0.2, -0.4, 1.3 -1.5) first cluster 3-cluster results may roughly equivalent third cluster 4-cluster results, . ’s worth also checking lists cluster members see clusters similar parameters similar lists members, can conclude part clustering model remained roughly consistent even another cluster added model. multiple sets results (example, results different models) want compare parameter estimates, simple way make bit consistent relabel parameters increasing order cluster main effect. ’re row clusters, relabel order increasing rowc values ’re column clusters, relabel order increasing colc values ...\\$parlist..","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"a-note-about-notation","dir":"Articles","previous_headings":"","what":"A note about notation","title":"`clustord` Tutorial","text":"looking cited journal articles Pledger Arnold (2014), Matechou et al. (2016), Fernández et al. (2016 2019), notation slightly different notation used tutorial. package tutorial notation changed reduce confusion parameters row clustering column clustering models. Table 2 glossary notation used clustord corresponding notation used articles. rest parameters retain names tutorial cited references. Note also , although theoretically possible model structure add αr\\alpha_r αi\\alpha_i model, ie. row cluster effects individual row effects, clustord allow , warn try use Y ~ ROWCLUST + ROW similar formulae. biclustering model, αr\\alpha_r βc\\beta_c, allow either individual row individual column effects, partly introduce many parameters difficult fit correctly.","code":""},{"path":"https://vuw-clustering.github.io/clustord/articles/clustordTutorial.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"`clustord` Tutorial","text":"Agresti, . (2010). Analysis ordinal categorical data. Vol. 656, John Wiley & Sons. Akaike, H. (1973). Maximum likelihood identification Gaussian autoregressive moving average models. Biometrika, 60(2), 255-265. Anderson, J. . (1984). Regression ordered categorical variables. Journal Royal Statistical Society – Series B (Methodological), pp. 1–30. Biernacki, C., Celeux, G., Govaert, G. (2000). Assessing mixture model clustering integrated completed likelihood. IEEE Trans. Pattern Analysis Machine Intelligence, 22(7), 719-725. Dempster, . P., Laird, N. M. Rubin, D. B. (1977). Maximum likelihood incomplete data via EM algorithm. Journal Royal Statistical Society. Series B (Methodological), 39, pp. 1–22. Fernández, D., Arnold., R. Pledger, S. (2016). Mixture-based clustering ordered stereotype model. Computational Statistics & Data Analysis, 93, pp. 46–75. Fernández, D., Arnold, R., Pledger, S., Liu, ., & Costilla, R. (2019). Finite mixture biclustering discrete type multivariate data. Advances Data Analysis Classification, 13, pp. 117–143. Jacques, J. Biernacki, C. (2018). Model-based co-clustering ordinal data. Computational Statistics & Data Analysis, 123, pp. 101–115. Lloyd, S. P. (1982). Least squares quantization PCM. IEEE Transactions Information Theory, 28(2), pp. 129–137. MacQueen, J. B. (1967). Methods classification Analysis Multivariate Observations. Proceedings 5th Berkeley Symposium Mathematical Statistics Probability. University California Press, 1(14), pp. 281–297. Matechou, E., Liu, ., Fernández, D. Farias, M., Gjelsvik, B. (2016). Biclustering models two-mode ordinal data. Psychometrika, 81, pp. 611–624. McLachlan, G. J. Basford, K. E. (1988) Mixture Models: Inference Applications Clustering. Marcel Dekker, New York. McLachlan, G. J. Krishnan, T. (2007). EM algorithm extensions, (Vol. 382). John Wiley & Sons. McLachlan, G. J. Peel, D. (2000). Finite Mixture Models, (Vol. 299). John Wiley & Sons. O’Neill, R. Wetherill, G. B. (1971). present state multiple comparison methods (discussion). Journal Royal Statistical Society (B), 33, pp. 218–250. Pledger, S. Arnold, R. (2014). Multivariate methods using mixtures: Correspondence analysis, scaling pattern-detection. Computational Statistics Data Analysis 71, pp. 241–261. Schwarz, G. E. (1978). Estimating dimension model. Annals Statistics, 6(2): 461–464, doi:10.1214/aos/1176344136.","code":""},{"path":"https://vuw-clustering.github.io/clustord/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Louise McMillan. Author, maintainer, copyright holder. Daniel Fernández Martínez. Author. Ying Cui. Author. Eleni Matechou. Author.","code":""},{"path":"https://vuw-clustering.github.io/clustord/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"McMillan L, Fernández Martínez D, Cui Y, Matechou E (2024). clustord: Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model Binary Model. R package version 1.2.1, https://vuw-clustering.github.io/clustord/. Fernández, D., Arnold, R., Pledger, S., Liu, ., & Costilla, R. (2019). Finite mixture biclustering discrete type multivariate data. Advances Data Analysis Classification, 13, 117-143.","code":"@Manual{,   title = {clustord: Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model or Binary Model},   author = {Louise McMillan and Daniel {Fernández Martínez} and Ying Cui and Eleni Matechou},   year = {2024},   note = {R package version 1.2.1},   url = {https://vuw-clustering.github.io/clustord/}, } @Article{,   title = {Finite mixture biclustering of discrete type multivariate data},   author = {Daniel Fern{'a}ndez and Richard Arnold and Shirley Pledger and Ivy Liu and Roy Costilla},   journal = {Advances in Data Analysis and Classification},   publisher = {Springer},   year = {2019},   volume = {13},   pages = {117--143},   url = {https://link.springer.com/article/10.1007/s11634-018-0324-3}, }"},{"path":"https://vuw-clustering.github.io/clustord/index.html","id":"clustord","dir":"","previous_headings":"","what":"Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model or Binary Model","title":"Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model or Binary Model","text":"Please install clustord GitHub using remotes::install_github(\"vuw-clustering/clustord\", dependencies = TRUE, build_vignettes = TRUE). build_vignettes = TRUE part ensures install vignettes well, available help learn use package. get error try install package using method, please also try installing pak package use pak::pkg_install(\"vuw-clustering/clustord\", dependencies = TRUE).","code":""},{"path":"https://vuw-clustering.github.io/clustord/index.html","id":"update-2025-01-rerunning-for-convergence","dir":"","previous_headings":"","what":"Update 2025-01 Rerunning For Convergence","title":"Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model or Binary Model","text":"Version 1.2.1 adds utility function rerun(). original clustord() run converge, can use function rerun finishing point .e. skip stage finding random starts etc. supply previous results object data frame, rerun() feed details original run back new run continues previous run finished. can also used rerun endpoint original results slightly changed dataset.","code":""},{"path":"https://vuw-clustering.github.io/clustord/index.html","id":"update-2024-12-speed-improvement","dir":"","previous_headings":"","what":"Update 2024-12 Speed Improvement","title":"Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model or Binary Model","text":"Version 1.2, update improves speed algorithm. latest version unit tested ensure consistency original.","code":""},{"path":"https://vuw-clustering.github.io/clustord/index.html","id":"update-2024-11-parallelization","dir":"","previous_headings":"","what":"Update 2024-11 Parallelization","title":"Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model or Binary Model","text":"clustering function clustord() now parallel_starts option distribute random starts cores available (.e. n-1 cores n number available machine, leave 1 core non-R system tasks). Set parallel_starts = TRUE clustord() use .","code":""},{"path":"https://vuw-clustering.github.io/clustord/index.html","id":"update-2022-03-covariates","dir":"","previous_headings":"","what":"Update 2022-03 Covariates","title":"Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model or Binary Model","text":"Version 1.1 package major update version 0.1. now capacity fit models including variety covariates, make consistent models can fitted clustglm. Note input arguments output components changed version 0.1. version backwards compatible, .e. able run scripts v0.1 using v1.1, changes make backwards-compatible mostly stylistic ones avoid using “.” notation. example, input argument constraint.sum.zero now become constraint_sum_zero. avoids potential confusion S3 methods. six top-level functions. main one clustord(), performs clustering, including row clustering, column clustering, biclustering. also auxiliary one, mat2df(), may need run clustord() create input data structure clustord() (see clustord() manual details). Two functions, calc.SE.rowcluster() calc.SE.bicluster(), designed run clustord(), calculate standard errors clustering parameters, needed. calc.cluster.comparisons() can used compare clustering results different runs different models, avoiding label-switching issue. package also contains function osm() performs regression ordinal response using Ordered Stereotype Model. function included package R packages able fit stereotype model regression, fit ordering constraint. osm() function based MASS::polr() intended used similar way, wish perform regression via proportional odds use , wish perform regression using flexible ordered stereotype model use osm() package. package manuals functions also vignettes. Clustering multiple vignettes, explain different aspects options. package documentation online vuw-clustering.github.io/clustord Articles section lists vignettes. need help, please email louise.mcmillan@vuw.ac.nz.","code":""},{"path":"https://vuw-clustering.github.io/clustord/index.html","id":"citations","dir":"","previous_headings":"","what":"Citations","title":"Clustering Ordinal Data Using Proportional Odds Model, Ordered Stereotype Model or Binary Model","text":"using clustering methods, please cite package (can fetch citation R using cite(\"clustord\")) also cite one following. OSM clustering methods, please cite: Fernández, D., Arnold, R., & Pledger, S. (2016). Mixture-based clustering ordered stereotype model. Computational Statistics & Data Analysis, 93, 46-75. using POM methods, please cite: Matechou, E., Liu, ., Fernández, D., Farias, M., & Gjelsvik, B. (2016). Biclustering models two-mode ordinal data. psychometrika, 81(3), 611-624. using Binary methods, please cite: Pledger, S., & Arnold, R. (2014). Multivariate methods using mixtures: Correspondence analysis, scaling pattern-detection. Computational Statistics & Data Analysis, 71, 241-261. ordered stereotype regression, please cite package also cite Anderson (1984) Anderson first proposed ordered stereotype model.","code":"@article{fernandez2016mixture,   title={Mixture-based clustering for the ordered stereotype model},   author={Fern{\\'a}ndez, Daniel and Arnold, Richard and Pledger, Shirley},   journal={Computational Statistics \\& Data Analysis},   volume={93},   pages={46--75},   year={2016},   publisher={Elsevier} } @article{matechou2016biclustering,   title={Biclustering models for two-mode ordinal data},   author={Matechou, Eleni and Liu, Ivy and Fern{\\'a}ndez, Daniel and Farias, Miguel and Gjelsvik, Bergljot},   journal={psychometrika},   volume={81},   number={3},   pages={611--624},   year={2016},   publisher={Springer} } @article{pledger2014multivariate,   title={Multivariate methods using mixtures: Correspondence analysis, scaling and pattern-detection},   author={Pledger, Shirley and Arnold, Richard},   journal={Computational Statistics \\& Data Analysis},   volume={71},   pages={241--261},   year={2014},   publisher={Elsevier} }"},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.SE.bicluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate standard errors of clustering parameters. — calc.SE.rowcluster","title":"Calculate standard errors of clustering parameters. — calc.SE.rowcluster","text":"Calculate SE parameters fitted using clustord.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.SE.bicluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate standard errors of clustering parameters. — calc.SE.rowcluster","text":"","code":"calc.SE.rowcluster(long.df, clust.out, optim.control = default.optim.control())  calc.SE.bicluster(long.df, clust.out, optim.control = default.optim.control())"},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.SE.bicluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate standard errors of clustering parameters. — calc.SE.rowcluster","text":"long.df data frame, long format, passed clustord. clust.clustord object. optim.control control list optim call within M step EM algorithm. See control list Details optim manual info.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.SE.bicluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate standard errors of clustering parameters. — calc.SE.rowcluster","text":"standard errors corresponding elements clust.$outvect.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.SE.bicluster.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate standard errors of clustering parameters. — calc.SE.rowcluster","text":"Use calc.SE.rowcluster calculate SE row clustering column clustering, calc.SE.bicluster calculate SE biclustering. Calculates SE running optimHess (see optim) incomplete-data log-likelihood find hessian fitted parameter values clustord. square roots diagonal elements negative inverse hessian standard errors parameters .e. SE <- sqrt(diag(solve(-optim.hess)). Note SE values calculated independent parameters. example, constraint row clustering parameters set constraint_sum_zero = TRUE, last row clustering parameter negative sum parameters, SE values calculated first RG-1 parameters, independent ones. applies similarly individual column effect coefficients, etc. function requires input output clustord, includes component outvect, final vector independent parameter values EM algorithm, correspond subset parameter values parlist..","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.SE.bicluster.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Calculate standard errors of clustering parameters. — calc.SE.rowcluster","text":"calc.SE.rowcluster(): SE rowclustering calc.SE.bicluster(): SE biclustering","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.cluster.comparisons.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate comparison measures between two sets of clustering results — calc.cluster.comparisons","title":"Calculate comparison measures between two sets of clustering results — calc.cluster.comparisons","text":"Given two sets posterior probabilities membership clusters, calculate three measures compare clustering memberships.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.cluster.comparisons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate comparison measures between two sets of clustering results — calc.cluster.comparisons","text":"","code":"calc.cluster.comparisons(ppr1, ppr2)"},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.cluster.comparisons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate comparison measures between two sets of clustering results — calc.cluster.comparisons","text":"ppr1 Posterior probabilities cluster membership, named ppr_m ppc_m output clustord. performed biclustering, ppr1 clustering results just one dimensions .e. just row clustering results, just column clustering results. rows ppr1 give entries clustered, column corresponds one cluster. ppr2 Posterior probabilities cluster membership different clustering run, compared ppr1.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.cluster.comparisons.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate comparison measures between two sets of clustering results — calc.cluster.comparisons","text":"list components: ARI: Adjusted Rand Index. NVI: Normalised Variation Information. NID: Normalised Information Distance.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.cluster.comparisons.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate comparison measures between two sets of clustering results — calc.cluster.comparisons","text":"three measures Adjusted Rand Index (ARI), Normalised Variation Information (NVI) Normalised Information Distance (NID). three measures documented ","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/calc.cluster.comparisons.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate comparison measures between two sets of clustering results — calc.cluster.comparisons","text":"Fernández, D., & Pledger, S. (2016). Categorising count data   ordinal responses application ecological communities. Journal   agricultural, biological, environmental statistics (JABES), 21(2),   348–362.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord-package.html","id":null,"dir":"Reference","previous_headings":"","what":"clustord: Clustering Using Proportional Odds Model, Ordered Stereotype Model or Binary Model. — clustord-package","title":"clustord: Clustering Using Proportional Odds Model, Ordered Stereotype Model or Binary Model. — clustord-package","text":"Biclustering, row clustering column clustering using proportional odds model (POM), ordered stereotype model (OSM) binary model ordinal categorical data.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"clustord: Clustering Using Proportional Odds Model, Ordered Stereotype Model or Binary Model. — clustord-package","text":"clustord package provides six functions: clustord(), rerun(), mat2df(), calc.SE.rowcluster(), calc.SE.bicluster(), calc.cluster.comparisons().","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord-package.html","id":"clustering-function","dir":"Reference","previous_headings":"","what":"Clustering function","title":"clustord: Clustering Using Proportional Odds Model, Ordered Stereotype Model or Binary Model. — clustord-package","text":"main function clustord(),   fits clustering model data. model fitted using   likelihood-based clustering via EM algorithm. package assumes   started data matrix responses, though need   convert data matrix long-form data frame running   clustord. Every element original data matrix becomes one   row data frame, row column indices data matrix   become columns ROW COL data frame. can perform   clustering rows columns data matrix, biclustering   rows columns simultaneously. can include number covariates   rows covariates columns. Ordinal models used package   Ordered Stereotype Model (OSM), Proportional Odds Model (POM)   dedicated Binary Model binary data. rerun() function useful continuing clustering runs converge first attempt, running new clustering runs using estimated parameters previous run starting point. main input function clustord object output clustord, internally rerun function runs clustord, setting input parameters based original model fitting run.#'","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord-package.html","id":"utility-function","dir":"Reference","previous_headings":"","what":"Utility function","title":"clustord: Clustering Using Proportional Odds Model, Ordered Stereotype Model or Binary Model. — clustord-package","text":"mat2df() utility function provided convert data matrix responses long-form data frame format required clustord(), can also attach covariates long-form data frame needed.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord-package.html","id":"se-calculation-functions","dir":"Reference","previous_headings":"","what":"SE calculation functions","title":"clustord: Clustering Using Proportional Odds Model, Ordered Stereotype Model or Binary Model. — clustord-package","text":"calc.SE.rowcluster() calc.SE.bicluster() functions run running clustord(), calculate standard errors parameters fitted using clustord().","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord-package.html","id":"clustering-comparisons","dir":"Reference","previous_headings":"","what":"Clustering comparisons","title":"clustord: Clustering Using Proportional Odds Model, Ordered Stereotype Model or Binary Model. — clustord-package","text":"calc.cluster.comparisons() can used compare assigned cluster memberships rows columns data matrix two different clustering fits, way avoids label-switching problem.","code":""},{"path":[]},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"clustord: Clustering Using Proportional Odds Model, Ordered Stereotype Model or Binary Model. — clustord-package","text":"Maintainer: Louise McMillan louise.mcmillan@vuw.ac.nz (ORCID) [copyright holder] Authors: Daniel Fernández Martínez daniel.fernandez.martinez@upc.edu (ORCID) Ying Cui ying.cui@sms.vuw.ac.nz Eleni Matechou e.matechou@kent.ac.uk (ORCID)","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional Odds Models (POM) or Binary Models — clustord","title":"Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional Odds Models (POM) or Binary Models — clustord","text":"Likelihood-based clustering parameters fitted using EM algorithm. can perform clustering rows columns data matrix, biclustering rows columns simultaneously. can include number covariates rows covariates columns. Ordinal models used package Ordered Stereotype Model (OSM), Proportional Odds Model (POM) dedicated Binary Model binary data.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional Odds Models (POM) or Binary Models — clustord","text":"","code":"clustord(   formula,   model,   nclus.row = NULL,   nclus.column = NULL,   long.df,   initvect = NULL,   pi.init = NULL,   kappa.init = NULL,   EM.control = default.EM.control(),   optim.method = \"L-BFGS-B\",   optim.control = default.optim.control(),   constraint_sum_zero = TRUE,   start_from_simple_model = TRUE,   parallel_starts = FALSE,   nstarts = 5,   verbose = FALSE )"},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional Odds Models (POM) or Binary Models — clustord","text":"formula model formula (see 'Details'). model \"OSM\" Ordered Stereotype Model \"POM\" Proportional Odds Model \"Binary\" binary data model. nclus.row number row clustering groups. nclus.column number column clustering groups. long.df data frame least three columns, Y ROW COL. row data frame corresponds single cell original data matrix; response value cell given Y, row column indices cell matrix given ROW COL. Use mat2df create data frame data matrix responses. mat2df also allows supply data frames row column covariates incorporated long.df. initvect (default NULL) vector starting parameter values model.     Note: user enters initial vector parameter values,     strongly recommend user also check values     parlist.init output object, make sure     constructed parameters expected. NULL, starting parameter values generated automatically. See 'Details' definitions parameters used different models. pi.init (default NULL) starting parameter values proportions     observations different row clusters. NULL, starting values generated automatically. User-specified values pi.init must length (nclus.row-1)     final value automatically calculated     values pi sum 1. kappa.init (default NULL) starting parameter values     proportions observations different column clusters. NULL, starting values generated automatically. User-specified values kappa.init must length     (nclus.column-1) final value automatically     calculated values kappa sum 1. EM.control (default = list(EMcycles=50, EMlikelihoodtol=1e-4,     EMparamtol=1e-2, paramstopping=TRUE, startEMcycles=10, keepallparams=FALSE,     epsilon=1e-6))     list parameters controlling EM algorithm. EMcycles controls many EM iterations main EM algorithm     used fit chosen submodel. EMlikelihoodtol tolerance stopping criterion     log-likelihood EM algorithm. criterion     absolute change incomplete log-likelihood since     previous iteration, scaled size dataset n*p,     n number rows data matrix p     number columns data matrix. scaling applied     incomplete log-likelihood predominantly affected dataset size. EMparamtol tolerance stopping criterion     parameters EM algorithm. tolerance     sum scaled parameter changes last iteration,     .e. tolerance individual parameter sum     changes parameters. Thus default tolerance 1e-2.     individual parameter criteria absolute differences     exponentiated absolute parameter value current timestep     exponentiated absolute parameter value previous timestep,     proportion exponentiated absolute parameter value current     timestep. exponentiation rescale parameter values     close zero. around 5 independent parameter values, point     convergence using default tolerances log-likelihood     parameters, parameter scaled absolute change since     previous iteration 1e-4; 20 30 independent     parameters, scaled aboslute change 1e-6. paramstopping: FALSE, indicates EM algorithm     check convergence based change incomplete-data     log-likelihood, relative current difference     complete-data incomplete-data log-likelihoods, .e.     abs(delta_lli)/abs(llc[iter] - lli[iter]);     TRUE, indicates well checking likelihood     criterion, EM algorithm also check whether relative change     exponentials absolute values current parameters     tolerance EMstoppingpar, see whether parameters     likelihood converged. startEMcycles controls many EM iterations used     fitting simpler submodels get starting values fitting models     interaction. keepallparams: true, keep record parameter values     (including pi_r kappa_c) every EM iteration. columnclustering, parameters saved iteration     converted column clustering format, row     clustering format, alpha     EM.status$params.every.iteration correspond beta_c     pi correspond kappa. epsilon: default 1e-6, small value used adjust values pi,     kappa theta close zero taking logs     create infinite values. optim.method (default \"L-BFGS-B\") method use optim within M step EM algorithm. Must one 'L-BFGS-B', 'BFGS', 'CG' 'Nelder-Mead' (.e. SANN method). optim.control control list optim call within M step EM algorithm. See control list Details optim manual info. Please note although optim, default, uses pgtol=0 factr=1e7 L-BFGS-B method, clustord, default, alters pgtol=1e-4 factr=1e11, can use optim.control argument clustord revert defaults want. reason change chosen values clustord reduce tolerance log-likelihood function optimization order speed algorithm, log-likelihood scale 1e4 <100 rows data matrix 1e6 5000 rows data matrix, tolerance default optim scale important choice model type structure number starting points. one model better another, probably likelihood better size data matrix, far larger tolerance optimization. one starting point better another, probably likelihood better 1/10th 1/100th size data matrix, still far larger tolerance optimization. need accurate parameter estimates, firstly make sure try starting points, perform model selection first, finally rerun chosen model finer tolerance, e.g. optim defaults, pgtol=0 factr=1e7. constraint_sum_zero (default TRUE) TRUE, use constraints cluster effects sum zero; FALSE, use constraints cluster effect first cluster 0. versions constraints joint row-column cluster effects: effects described matrix parameters gamma_rc, indexed row cluster column cluster indices, constraints final column gamma_rc equal negative sum columns (gamma columns sum zero) first row gamma_rc equal negative sum rows (gamma rows sum zero). start_from_simple_model (default TRUE) TRUE, fit simpler clustering model first use provide starting values parameters model interactions; FALSE, use basic models provide starting values pi.init kappa.init. full model interaction terms, simpler models ones without interactions. model individual row/column effects alongside clusters, simpler models ones without individual row/column effects. full model covariates, simpler models ones without covariates (get starting values cluster parameters), ones covariates clustering (get starting values covariates). parallel_starts (default FALSE) TRUE, generating multiple random starts, random starts parallelised many cores available. example, personal computer one fewer number cores machine, make sure one left system tasks external R. nstarts (default 5) number random starts generate, generating random starting points EM algorithm. verbose (default FALSE) changes much reported console algorithm's progress. TRUE, reports incomplete-data log-likelihood every EM algorithm iteration trace information nnet::multinom() process fitting initial values mu parameters. FALSE, incomplete log-likelihood reported every 10 iterations EM algorithm initial fitting reporting suppressed. Regardless verbosity setting algorithm reports random starts whenever finds better log-likelihood previous starts, also reports fitting simpler models find starting values parameters vs fitting final, complex model. wanting detailed output optim(), use optim.control=list(trace=X), X 1 6, 6 highest level verbosity L-BFGS-B algorithm.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional Odds Models (POM) or Binary Models — clustord","text":"clustord object, .e. list components: info: Basic info n, p, q, number parameters, number     row clusters number column clusters, relevant. model: model used fitting, \"OSM\" Ordered Stereotype     Model, \"POM\" Proportional Odds Model, \"Binary\" Binary model. EM.status: list containing latest iteration iter,     latest incomplete-data complete-data log-likelihoods new.lli     new.llc, best incomplete-data log-likelihood best.lli     corresponding complete-data log-likelihood, llc..best.lli,     parameters best incomplete-data log-likelihood,     params..best.lli, indicator whether algorithm converged     converged, user chose keep parameter values     every iteration, also params.every.iteration. Note biclustering, .e. ROWCLUST     COLCLUST included model, incomplete     log-likelihood calculated using entropy approximation,     may inaccurate unless algorithm converged close     converging. beware using incomplete log-likelihood     corresponding AIC value unless EM algorithm converged. criteria: calculated values AIC, BIC,     etc. best incomplete-data log-likelihood. epsilon: small value (default 1e-6) used adjust values     pi kappa theta close zero, taking logs     produce infinite values. Use EM.control argument     adjust epsilon. constraints_sum_zero: chosen value constraints_sum_zero. param_lengths: vector total number parameters/coefficients     part model, labelled names components.     value 0 component included model, e.g.     covariates interacting row clusters     rowc_cov_coef value 0. component included,     value given include dependent parameter/coefficient values,     column clusters included colc_coef value     nclus.column, whereas number independent values     nclus.column - 1. initvect: initial vector parameter values, either     specified user generated automatically. vector     independent values parameters, full set. outvect: final vector parameter values, containing     independent parameter values parlist.. parlist.init: initial list parameters, constructed     initial parameter vector initvect. Note initial     vector incorrectly specified, values parlist.init     may expected, checked user. parlist.: fitted values parameters. pi, kappa: fitted values pi kappa, relevant. ppr, ppc: posterior probabilities membership     row clusters column clusters, relevant. rowc_mm, colc_mm, cov_mm: model matrices ,     respectively, covariates interacting row clusters, covariates     interacting column clusters, covariates interacting     row column clusters (.e. covariates constant coefficients).     Note one row model matrix corresponds one row long.df. RowClusters, ColumnClusters: assigned row column     clusters, relevant, row/column assigned cluster     based maximum posterior probability cluster membership (ppr     ppc). RowClusterMembers, ColumnClusterMembers: vectors     assigned members row column cluster, row/column     assigned cluster based maximum posterior probability cluster     membership (ppr ppc)","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional Odds Models (POM) or Binary Models — clustord","text":"can select input parameters, starting values generated running kmeans fitting simpler models feeding outputs final model starting values. starting point clustering data matrix response values binary categorical. may also data frame covariates linked rows data matrix, may also data frame covariates linked columns data matrix. example, clustering data fishing trawls, rows trawl events columns species caught, also supply gear covariate linked rows, representing gear used trawl event, additionally supply species covariates linked columns, representing auxiliary information species. requirement provide covariates, can provide row covariates, column covariates. running clustord, need run mat2df convert data matrix long form data frame. data frame needs least three columns, Y ROW COL. row data frame corresponds single cell original data matrix; response value cell given Y, row column indices cell matrix given ROW COL. mat2df also allows supply data frames row column covariates incorporated long.df. , run clustord function, need enter chosen formula model, number clusters want fit. formula model_structure akin glm, restrictions. can include number covariates way multiple regression model, though unlike glm, can include row column covariates. Note , unlike glm, specify family argument; model argument used instead. formula argument details following description different models, Binary model used simplicity giving mathematical descriptions models, can use following models Ordered Stereotype Proportional Odds Models well. formula argument, response must exactly Y. use functions Y response, can include Y terms right hand side formula. Y name clustord response values original data matrix. formula argument 4 special variables: ROWCLUST, COLCLUST, ROW COL. restrictions can used formula, covariates, instead act indicators clustering model_structure want use. variables formula covariates want include model, unrestricted, can used way glm. ROWCLUST COLCLUST used indicate row clustering model_structure want, column clustering model_structure want, respectively. inclusion ROWCLUST single term indicates want include row clustering effect model. simplest row clustering model, Binary data row clustering effects , basic function call clustord(Y ~ ROWCLUST, model=\"Binary\", long.df=long.df) model fitted form: Logit(P(Y = 1)) = mu + rowc_coef_r mu intercept term, rowc_coef_r row cluster effect applied every row original data matrix member row cluster r. inclusion ROWCLUST corresponds inclusion rowc_coef_r. Note using notation involving greek letters, () ran letters different types parameters model (b) many parameters, difficult remember ones . Similarly row clustering, formula Y ~ COLCLUST perform column clustering, model Logit(P(Y = 1)) = mu + colc_coef_c, colc_coef_c column cluster effect applied every column original data matrix member column cluster c. Including ROWCLUST COLCLUST formula indicates want perform biclustering, .e. want cluster rows columns original data matrix simultaneously. included without interaction, terms just correspond including rowc_coef_r colc_coef_c model: formula Y ~ ROWCLUST + COLCLUST simplest possible biclustering model, Logit(P(Y = 1)) = mu + rowc_coef_r + colc_coef_c want include interaction rows columns, .e. want perform block biclustering block corresponds row cluster r column cluster c, model matrix parameters indexed r c. clustord(Y ~ ROWCLUST*COLCLUST, model=\"Binary\", ...) model Logit(P(Y = 1)) = mu + rowc_colc_coef_rc model can instead called using equivalent formula Y ~ ROWCLUST + COLCLUST + ROWCLUST:COLCLUST. can instead use formula Y ~ ROWCLUST:COLCLUST. Mathematically, equivalent previous two. regression, models equivalent clustering, equivalent, number independent parameters overall. include main effects, reduces number independent parameters interaction term compared just use interaction term (see section initvect). include just one main effects alongside interaction term, .e. use Y ~ ROWCLUST + ROWCLUST:COLCLUST Y ~ COLCLUST + ROWCLUST:COLCLUST. simplicity code, avoid confusion interpreting results. However, clustord allows lot flexibility . variables ROW COL used indicate want also include individual row column effects, respectively. example, clustering binary data indicates presence/ absence different species (columns) different trawl events (rows), know one particular species incredibly common, can include column effects model, allow possibility two columns may correspond species different probabilities appearing trawl. can add individual column effects along row clustering, can add individual row effects along column clustering. formula row clustering individual column effects (without interaction) Y ~ ROWCLUST + COL corresponds Binary model Logit(P(Y = 1)) = mu + rowc_coef_r + col_coef_j two cells data matrix row cluster, different columns, probability Y = 1. can also add interaction individual row/column effects clustering effects. still want able see row cluster column effects separately, use Y ~ ROWCLUST*COL Y ~ ROWCLUST + COL + ROWCLUST:COL (), model Logit(P(Y = 1)) = mu + rowc_coef_r + col_coef_j + rowc_col_coef_rj , rowc_coef_r col_coef_j row cluster effects individual column effects, rowc_col_coef_rj interaction terms. Alternatively, can use mathematically-equivalent formula Y ~ ROWCLUST:COL model Logit(P(Y = 1)) = mu + rowc_col_coef_rj row cluster effects individual column effects absorbed matrix rowc_col_coef_rj. models mathematically, differences constrained (see section initvect argument) interpreted. Note using covariates, equivalent leave main effects just use interaction terms, clustering models work quite regression models covariates. Equivalently, want cluster columns, can include individual row effects alongside column clusters, .e. Y ~ COLCLUST + ROW Y ~ COLCLUST + ROW + COLCLUST:ROW, depending whether want interaction terms . able include individual row effects row clusters, include individual column effects column clusters, enough information ordinal binary data fit models. consequence, include individual row column effects biclustering, e.g. Y ~ ROWCLUST + COLCLUST + ROW Y ~ ROwCLUST + COLCLUST + COL permitted. version 1 package, can now also include covariates alongside clustering patterns. basic way include additions clustering model_structure. example, including one row covariate xr row clustering model formula Y ~ ROWCLUST + xr Binary model Logit(P(Y = 1)) = mu + rowc_coef_r + row_coef_1*xr_i row_coef_1 coefficient xr_i, just typical regression model. Additional row covariates can also included, can include interactions , functions , regression models, e.g. Y ~ ROWCLUST + xr1*log(xr2) Binary model Logit(P(Y = 1)) = mu + rowc_coef_r + row_coef1*xr1_i + row_coef2*log(xr2_i) +                          row_coef3*xr1_i*log(xr2_i) instead want add column covariates model, work way added long.df data frame using mat2df, indexed j instead . Simplest model, one single column covariate xc, formula Y ~ ROWCLUST + xc Binary model Logit(P(Y = 1)) = mu + rowc_coef_r + col_coef1*xc_j can use functions interactions column covariates, just row covariates. can similarly add row column covariates column clustering biclustering models. can include interactions covariates ROWCLUST COLCLUST formula. quite interactions covariates. formula Y ~ ROWCLUST*xr xr row covariate, corresponds Binary model Logit(P(Y = 1)) = mu + rowc_coef_r + cov_coef*xr_i + rowc_row_coef_r1*xr_i means term linear predictor involves row covariate xr (index row covariate), cluster (indexed r) different coefficient covariate (distinct non-interaction covariate models , coefficients covariates regardless cluster row ). different interaction terms involving covariates, two covariates appear multiplied together model shared coefficient term attached . clustering/covariate interaction, row column clustering pattern controls coefficients rather adding different type covariate. Note pure cluster effect rowc_coef_r also included model automatically, way regression formula Y ~ x1*x2 include individual x1 x2 effects well interaction x1 x2. coefficients row clusters interacting row coefficients named row_cluster_row_coef output clustord can also coefficients interactions row clustering column covariates, column clustering row covariates, column clustering column covariates. Row clustering interacting column covariates look something like Y ~ ROWCLUST*xc Binary model Logit(P(Y = 1)) = mu + rowc_coef_r + rowc_col_coef_r1*xc_j combinations clustering covariates work similarly. rowc_col_coef_rl similar coefficients two indices. first index index cluster, second index index covariate among list covariates interacting direction clustering. two row covariates xr1 xr2 interacting three row clusters, gives 6 coefficients: rowc_col_coef_11, rowc_col_coef_12, rowc_col_coef_21, rowc_col_coef_22, rowc_col_coef_31, rowc_col_coef_32. can also three-way interaction row cluster two covariates, add coefficients rowc_col_coef_r3 xr1:xr2 term. can instead add covariates interact column clusters, parameters colc_row_coef_cm, m indexes just covariates interacting column cluster. covariates interacting row clusters covariates interacting column clusters, parameters rowc_cov_coef_rl colc_cov_coef_cm. example model Y ~ ROWCLUST + xr1 + ROWCLUST:xr1 + xc1 + COLCLUST + COLCLUST:log(xc1) main effects row clusters column clusters, .e. ROWCLUST COLCLUST. also two covariate terms interacting clusters, xr1 xc1. also 1 covariate term interacting row clusters, xr1, coefficients rowc_cov_coef_r1, 1 covariate term interacting column clusters, log(xc1), coefficients colc_cov_coef_c1. Restrictions formula primary restriction formula argument use functions ROW, COL, ROWCLUST COLCLUST, log(ROW) (COLCLUST^2). covariates, manipulated like ; instead, indicators particular elements clustering model_structure. performing biclustering, .e. ROWCLUST COLCLUST model, want include interaction , can use interaction , can include main effects, allowed use just one main effect alongside interaction. , can use Y ~ ROWCLUST + COLCLUST + ROWCLUST:COLCLUST Y ~ ROWCLUST*COLCLUST, can use Y ~ ROWCLUST:COLCLUST, two types biclustering model different parameter constraints (see initvect details), use Y ~ ROWCLUST + ROWCLUST:COLCLUST Y ~ COLCLUST + ROWCLUST:COLCLUST stated , also include individual row effects alongside row clustering, use individual column effects alongside column clustering, .e. ROWCLUST formula, ROW cannnot formula, COLCLUST formula COL formula. including COL ROWCLUST, can include interaction permitted interaction term involves COL, similarly interaction ROW COLCLUST permitted interaction term involves ROW. can include interactions form Y ~ ROWCLUST + COL + ROWCLUST:COL Y ~ ROWCLUST*COL, Y ~ ROWCLUST:COL. permitted uses COL term, equivalent constraints inclusion ROW. stated , can include interactions ROWCLUST COLCLUST covariates, include three-way interactions ROWCLUST, COLCLUST one covariates permitted clustord, mostly prohibitive number parameter values need fitted, difficulty interpreting model. , use formulae Y ~ ROWCLUST*COLCLUST*xr, Binary model Logit(P(Y = 1)) = mu + bi_cluster_row_coef_rc1*xr_i. model argument details three models available clustord Binary model, Bernoulli model equivalent binary model package clustglm, Proportional Odds Model (POM) Ordered Stereotype Model (OSM). Many Binary model examples given , general form logit(P(Y = 1)) = mu + <<linear terms>> linear terms can include row column clustering effects, individual row column effects, row column covariates, without interactions row column clustering. Proportional Odds Model Ordered Stereotype Model model_structure linear terms, overall model equation different. Proportional Odds Model (model = \"POM\") form logit(P(Y <= k)) = log(P(Y <= k)/P(Y > k)) = mu_k - <<linear terms>> simplest POM row clustering logit(P(Y <= k)) = mu_k - rowc_coef_r model including individual column effects interactions logit(P(Y <= k)) = mu_k - rowc_coef_r - col_coef_j Note linear-term coefficients negative signs Proportional Odds Models. row cluster index increases, column index increases, Y likely fall higher values (see Ch4 Agresti, 2010). Ordered Stereotype model (model = \"OSM\") form log(P(Y = k)/P(Y = 1)) = mu_k + phi_k(<<linear terms>>) simplest OSM row clustering log(P(Y = k)/P(Y = 1)) = mu_k + phi_k*rowc_coef_r model including individual column effects interactions log(P(Y = k)/P(Y = 1)) = mu_k + phi_k(rowc_coef_r + col_coef_j) Note OSM cumulative logit model, unlike POM. model describes log kth level relative first level, baseline category, patterns k = 2 may different patterns k = 3. linked, linear terms , may shape. sense, OSM flexible/less restrictive POM. See Anderson (1984) original definition ordered stereotype model, see Fernández et al. (2016) application clustering. phi_k parameters may treated \"score\" parameters. fitting OSM, fitted phi_k values can give indication true separation categories. Even default labelling categories 1 n, mean categories actually equally spaced reality. fitted phi_k values OSM can treated data-derived numerical labels categories. Moreover, two categories similar fitted phi_k values, e.g. phi_2 = 0.11 phi_3 = 0.13, suggests enough information data distinguish categories 2 3, might well merge single category simplify model-fitting process interpretation results. initvect argument details Initvect vector starting values parameters, made sections different type parameter model. Note length section initvect number independent parameter values, overall number parameter values type. want supply vector starting values EM algorithm, need careful many values supply, order include initvect, CHECK output list parameters (full set parameter values, including dependent ones, broken type parameter) check initvect model_structure correct formula specified. example, number mu values always 1 fewer number categories data, remaining value mu dependent q-1 values. OSM data 3 categories, first value mu category 1 0, 2 values mu categories 2 3 independent values mu. POM data 5 categories, first 4 values mu independent values last value mu infinity, probability Y category 5 defined 1 minus sum probabilities 4 levels. q number levels values y, n number rows original data matrix, p number columns original data matrix. Binary, one independent value mu, .e. q = 2. Ignore phi, used Binary model. OSM, starting values mu_k length q-1, model mu_1 = 0 always, initvect values mu become mu_2, mu_3, etc. mu_q. starting values phi_k length q-2. Note starting values phi correspond directly phi, phi restricted increasing 0 1, instead starting values treated elements u[2:q-1] vector u can -Inf +Inf, phi[2] <- expit(u[2]) phi[k] <- expit(u[2] + sum(exp(u[3:k]))) k 3 q-1 (phi[1] = 0 phi[q] = 1). POM, starting values mu_k length q-1, starting values correspond directly mu_k, mu_k restricted increasing, .e. model     mu_1 <= mu_2 <= ... mu_q = +Inf instead using initvect values directly mu_k, 2nd (q-1)th elements initvect used construct mu_k follows: mu_1 <- initvect[1] mu_2 <- initvect[1] + exp(initvect[2]) mu_3 <- initvect[1] + exp(initvect[2]) + exp(initvect[3]) ... mu_{k-1}, mu_k infinity,     used directly construct probability Y = q. Thus values used construct mu_k can unconstrained, makes easier specify initvect easier optimize parameter values. Ignore phi, used POM. three models, starting values rowc_coef_r length nclus.row-1, nclus.row number row clusters. final row cluster parameter dependent others (see input parameter info constraint_sum_zero), whereas independent colinear mu_k parameters thus identifiable. Similarly starting values colc_coef_c length nclus.column-1, nclus.column number column clusters, avoid problems colinearity nonidentifiability. biclustering interaction term row clusters column clusters, number independent values matrix interaction terms depends whether include main effects row column clusters separately. , use biclustering model Y ~ ROWCLUST + COLCLUST + ROWCLUST:COLCLUST, equivalently Y ~ ROWCLUST*COLCLUST, main effect term ROWCLUST nclus.row-1 independent parameters initvect, COLCLUST nclus.column-1 independent parameters initvect, ROWCLUST:COLCLUST (nclus.row - 1)*(nclus.column - 1) independent parameter values. final matrix interaction terms constrained last row equal negative sum rows, last column equal negative sum columns. hand, want use interaction term main effects (clustering model mathematically equivalent), .e. Y ~ ROWCLUST:COLCLUST, matrix interaction terms nclus.row*nclus.column - 1 independent parameters, .e. independent parameters included main effects. column effects alongside row clusters (permitted alongside column clusters), without interactions, .e. formula Y ~ ROWCLUST + COL Binary model Logit(P(Y = 1)) = mu + rowc_coef_r + col_coef_j row cluster coefficients nclus.row - 1 independent parameters, column effect coefficients p - 1 independent parameters, p number columns original data matrix, .e. maximum value long.df$COL. include interaction term, number independent parameters depends whether just use interaction term, include main effects. formula Y ~ ROWCLUST + COL + ROWCLUST:COL equivalent \"*\", interaction term (nclus.row - 1)*(p-1) independent parameters. instead use formula Y ~ ROWCLUST:COL, interaction term nclus.row*p - 1 independent parameters. Either way, total number independent parameters model nclus.row*p. Similarly, row effects alongside column clusters, without interactions, .e. formula Y ~ COLCLUST + ROW, Binary model Logit(P(Y = 1)) = mu + colc_coef_c + row_coef_i column cluster coefficients nclus.column - 1 independent parameters, row coefficients n-1 independent parameters, n number rows original data matrix, .e. maximum value long.df$ROW. include interaction term alongside main effects, .e. Y ~ COLCLUST + ROW + COLCLUST:ROW, equivalent \"*\", interaction term (nclus.column - 1)*(n-1) independent parameters. instead use formula Y ~ COLCLUST:ROW, interaction coefficient matrix nclus.column*n - 1 independent parameters. covariate terms included formula split clustord covariates interact row clusters, covariates interact column clusters, covariates interact row column clusters. number independent parameters row-cluster-interacting covariates nclus.row*L, L number terms involving row clusters covariates \"*\" terms expanded. formula, example, Y ~ ROWCLUST*xr1 + xr2 + ROWCLUST:log(xc1) xr1 xr2 row covariates, xc1 column covariate, fully expanded formula Y ~ ROWCLUST + xr1 + xr2 + ROWCLUST:xr1 + ROWCLUST:log(xc1) terms interacting ROWCLUST ROWCLUST:xr1 ROWCLUST:log(xc1), nclus.row*2 independent coefficients covariates. number independent parameters column-cluster-interacting covariates nclus.column*M, M number terms involving column clusters covariates \"*\" terms expanded. formula, example, Y ~ (xr1^2) + COLCLUST*xc1 + COLCLUST:xc2:xc3 + COLCLUST*xr1 expanded Y ~ COLCLUST + xr1 + (xr1^2) + xc1 + COLCLUST:xc1 + COLCLUST:xc2:xc3 + COLCLUST:xr1 terms interacting COLCLUST COLCLUST:xc1, COLCLUST:xc2:xc3 COLCLUST:xr1, nclus.column*3 independent coefficients covariates. number independent parameters covariates interact row column clusters number covariate terms, \"*\" terms expanded. formula, example, Y ~ ROWCLUST*xr1 + xr2 + ROWCLUST:log(xc1) + COLCLUST*xc1 expanded Y ~ ROWCLUST +COLCLUST + xr1 + xr2 + xc1 + ROWCLUST:xr1 + ROWCLUST:log(xc1) + COLCLUST:xc1, 3 independent coefficients terms xr1, xr2, xc1. Note intercept terms coefficients, incorporated parameters mu_k. order initvect entries follows, entries included formula ignored included initvect. , provide values initvect components included formula. 1) mu (values used construct mu, POM ) 2) values used construct phi (OSM ) 3) row cluster coefficients 4) column cluster coefficients 5) [matrix] bicluster coefficients (.e. interaction row column clusters) 6) individual row coefficients 7) individual column coefficients 8) [matrix] interactions row clusters individual column coefficients 9) [matrix] interactions column clusters individual row coefficients 10) [matrix] row-cluster-specific coefficients covariates interacting row clusters 11) [matrix] column-cluster-specific coefficients covariates interacting column clusters 12) coefficients covariates interact row column clusters entries marked [matrix] constructed matrices filling matrices row-wise, e.g. want starting values 1:6 matrix 2 row clusters 3 covariates interacting row clusters, matrix coefficients become 1 2 3 4 5 6 formula Y ~ ROWCLUST*COLCLUST, matrix interactions row column clusters (nclus.row - 1)*(nclus.column - 1) independent parameters, last row column matrix negative sums rest, e.g. 2 row clusters 3 column clusters, 2 independent values, provide starting values -0.5 1.2, final matrix parameters : column cluster 1   column cluster 2   column cluster 3 row cluster 1   -0.5               1.2                -0.7 row cluster 2   0.5                -1.2               0.7 matrix matrix relating row clusters, row clusters rows, matrix relating column clusters row clusters, column clusters rows, .e. matrix coefficients column clusters interacting individual row effects rows matrix corresponding clusters, .e. matrix indexed colc_row_coef_ci, c column cluster index row index. Similarly, matrix matrix relating column clusters covariates, rows matrix correspond column clusters, .e. matrix indexed colc_cov_coef_cl, c column cluster index l covariate index. using biclustering interaction row column clusters, row clusters rows column clusters columns, .e. matrix indexed rowc_colc_coef_rc, r row cluster index c column cluster index.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional Odds Models (POM) or Binary Models — clustord","text":"Fernandez, D., Arnold, R., & Pledger, S. (2016). Mixture-based clustering ordered stereotype model. Computational Statistics & Data Analysis, 93, 46-75. Anderson, J. . (1984). Regression ordered categorical variables. Journal Royal Statistical Society: Series B (Methodological), 46(1), 1-22. Agresti, . (2010). Analysis ordinal categorical data (Vol. 656). John Wiley & Sons.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/clustord.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood-based clustering using Ordered Stereotype Models (OSM), Proportional Odds Models (POM) or Binary Models — clustord","text":"","code":"set.seed(1) long.df <- data.frame(Y=factor(sample(1:3,5*20,replace=TRUE)),                ROW=factor(rep(1:20,times=5)),COL=rep(1:5,each=20))  # Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*rowc_coef_r with 3 row clustering groups: clustord(Y~ROWCLUST,model=\"OSM\",3,long.df=long.df,              EM.control=list(EMcycles=2,startEMcycles=2), nstarts=2) #> [1] \"Converting factor ROW to numeric.\" #> [1] \"EM algorithm for OSM\" #> Randomly generated start #1 #> Found better incomplete log-like: -110.223835125182  #> Randomly generated start #2 #> Found better incomplete log-like: -109.564729537795  #> [1] \"EM algorithm has not converged. Please try again, or with a different random seed, or with more starting points.\" #>  #> Call: #> clustord(formula = Y ~ ROWCLUST, model = \"OSM\", nclus.row = 3,  #>     long.df = long.df, EM.control = list(EMcycles = 2, startEMcycles = 2),  #>     nstarts = 2) #>  #> Clustering mode: #>  row clustering #>  #> Converged: #>  FALSE #>  #> Cluster sizes: #> Row clusters:  2 18 0   # Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*(rowc_coef_r + col_coef_j) with 3 row clustering groups: clustord(Y~ROWCLUST+COL,model=\"OSM\",3,long.df=long.df,              EM.control=list(EMcycles=2,startEMcycles=2), nstarts=2) #> [1] \"Converting factor ROW to numeric.\" #> [1] \"EM algorithm for OSM\" #> Randomly generated start #1 #> Using the output of simpler model as initial values for full model #> === End of initial row-cluster-only model fitting === #> Found better incomplete log-like: -108.357375627435  #> Randomly generated start #2 #> Using the output of simpler model as initial values for full model #> === End of initial row-cluster-only model fitting === #> Found better incomplete log-like: -108.296822888244  #> [1] \"EM algorithm has not converged. Please try again, or with a different random seed, or with more starting points.\" #>  #> Call: #> clustord(formula = Y ~ ROWCLUST + COL, model = \"OSM\", nclus.row = 3,  #>     long.df = long.df, EM.control = list(EMcycles = 2, startEMcycles = 2),  #>     nstarts = 2) #>  #> Clustering mode: #>  row clustering #>  #> Converged: #>  FALSE #>  #> Cluster sizes: #> Row clusters:  0 3 17   # Model Logit(P(Y <= k))=mu_k-rowc_coef_r-col_coef_j-rowc_col_coef_rj with 2 row clustering groups: clustord(Y~ROWCLUST*COL,model=\"POM\",nclus.row=2,long.df=long.df,              EM.control=list(EMcycles=2,startEMcycles=2), nstarts=2) #> [1] \"Converting factor ROW to numeric.\" #> [1] \"EM algorithm for POM\" #> Randomly generated start #1 #> Using the output of simpler model as initial values for full model #> === End of initial row-cluster-only model fitting === #> Using the output of intermediate model as initial values for full model #> === End of intermediate rowcluster-column model fitting === #> Found better incomplete log-like: -108.74422639491  #> Randomly generated start #2 #> Using the output of simpler model as initial values for full model #> === End of initial row-cluster-only model fitting === #> Using the output of intermediate model as initial values for full model #> === End of intermediate rowcluster-column model fitting === #> Found better incomplete log-like: -108.734320154894  #> [1] \"EM algorithm has not converged. Please try again, or with a different random seed, or with more starting points.\" #>  #> Call: #> clustord(formula = Y ~ ROWCLUST * COL, model = \"POM\", nclus.row = 2,  #>     long.df = long.df, EM.control = list(EMcycles = 2, startEMcycles = 2),  #>     nstarts = 2) #>  #> Clustering mode: #>  row clustering #>  #> Converged: #>  FALSE #>  #> Cluster sizes: #> Row clusters:  20 0   # Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*(colc_coef_c) with 3 column clustering groups: clustord(Y~COLCLUST,model=\"OSM\",nclus.column=3,long.df=long.df,              EM.control=list(EMcycles=2,startEMcycles=2), nstarts=2) #> [1] \"Converting factor ROW to numeric.\" #> [1] \"EM algorithm for OSM\" #> Randomly generated start #1 #> Found better incomplete log-like: -109.524420500131  #> Randomly generated start #2 #> Found better incomplete log-like: -109.492519156351  #> [1] \"EM algorithm has not converged. Please try again, or with a different random seed, or with more starting points.\" #>  #> Call: #> clustord(formula = Y ~ COLCLUST, model = \"OSM\", nclus.column = 3,  #>     long.df = long.df, EM.control = list(EMcycles = 2, startEMcycles = 2),  #>     nstarts = 2) #>  #> Clustering mode: #>  column clustering #>  #> Converged: #>  FALSE #>  #> Cluster sizes: #> Column clusters:  5 0 0   # Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*(colc_coef_c + row_coef_i) with 3 column clustering groups: clustord(Y~COLCLUST+ROW,model=\"OSM\",nclus.column=3,long.df=long.df,              EM.control=list(EMcycles=2,startEMcycles=2), nstarts=2) #> [1] \"Converting factor ROW to numeric.\" #> [1] \"EM algorithm for OSM\" #> Randomly generated start #1 #> Using the output of simpler model as initial values for full model #> === End of initial row-cluster-only model fitting === #> Found better incomplete log-like: -167.029916889152  #> Randomly generated start #2 #> Using the output of simpler model as initial values for full model #> === End of initial row-cluster-only model fitting === #> Found better incomplete log-like: -104.673049751169  #> [1] \"EM algorithm has not converged. Please try again, or with a different random seed, or with more starting points.\" #>  #> Call: #> clustord(formula = Y ~ COLCLUST + ROW, model = \"OSM\", nclus.column = 3,  #>     long.df = long.df, EM.control = list(EMcycles = 2, startEMcycles = 2),  #>     nstarts = 2) #>  #> Clustering mode: #>  column clustering #>  #> Converged: #>  FALSE #>  #> Cluster sizes: #> Column clusters:  5 0 0   if (FALSE) { # \\dontrun{ # Model Log(P(Y=k)/P(Y=1))=mu_k+phi_k*(rowc_coef_r + colc_coef_c) #    with 3 row clustering groups and 2 column clustering groups: clustord(Y~ROWCLUST+COLCLUST,model=\"OSM\",nclus.row=3,nclus.column=2,long.df=long.df,              EM.control=list(EMcycles=2), nstarts=1)  # Model Logit(P(Y<=k))=mu_k-rowc_coef_r-colc_coef_c-rowc_colc_coef_rc #    with 2 row clustering groups and 4 column clustering groups, and #    interactions between them: clustord(Y~ROWCLUST*COLCLUST, model=\"POM\", nclus.row=2, nclus.column=4,              long.df=long.df,EM.control=list(EMcycles=2), nstarts=1,              start_from_simple_model=FALSE) } # }"},{"path":"https://vuw-clustering.github.io/clustord/reference/mat2df.html","id":null,"dir":"Reference","previous_headings":"","what":"Converting matrix of responses into a long-form data frame and incorporating covariates, if supplied. — mat2df","title":"Converting matrix of responses into a long-form data frame and incorporating covariates, if supplied. — mat2df","text":"Converting matrix responses long-form data frame incorporating covariates, supplied.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/mat2df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converting matrix of responses into a long-form data frame and incorporating covariates, if supplied. — mat2df","text":"","code":"mat2df(mat, xr.df = NULL, xc.df = NULL)"},{"path":"https://vuw-clustering.github.io/clustord/reference/mat2df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converting matrix of responses into a long-form data frame and incorporating covariates, if supplied. — mat2df","text":"mat matrix responses clustered xr.df optional data frame covariates corresponding rows mat. row xr.df corresponds one row mat, column xr.df covariate. xc.df optional data frame covariates corresponding columns mat. row xc.df corresponds one column mat, column xc.df covariate.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/mat2df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Converting matrix of responses into a long-form data frame and incorporating covariates, if supplied. — mat2df","text":"data frame columns Y, ROW COL,    additional columns covariates xr.df xc.df,    included. Y column output contains entries mat,    one row output per one cell mat, ROW    COL entries indicate row column data matrix    correspond given cell. cells NA left    output data frame. xr.df supplied, additional columns    output corresponding columns xr.df, values    covariate repeated every entry corresponding    row data matrix. Similarly, xc.df supplied, additional columns    output corresponding columns xc.df, values    covariate repeated every entry    corresponding column data matrix.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/osm.html","id":null,"dir":"Reference","previous_headings":"","what":"Ordinal data regression using the Ordered Stereotype Model (OSM). — osm","title":"Ordinal data regression using the Ordered Stereotype Model (OSM). — osm","text":"Fit regression model ordered factor response. model logistic probit model link function logit, link function log-based.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/osm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ordinal data regression using the Ordered Stereotype Model (OSM). — osm","text":"","code":"osm(   formula,   data,   weights,   start,   ...,   subset,   na.action,   Hess = FALSE,   model = TRUE )"},{"path":"https://vuw-clustering.github.io/clustord/reference/osm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ordinal data regression using the Ordered Stereotype Model (OSM). — osm","text":"formula formula expression regression models, form response ~ predictors. response factor (preferably ordered factor), interpreted ordinal response, levels ordered factor. model must intercept: attempts remove one lead warning ignored. offset may used. See documentation formula details. data data frame, list environment interpret variables occurring formula. weights optional case weights fitting. Default 1. start initial values parameters. See Details section information argument. ... additional arguments passed optim, often control argument. subset expression saying subset rows data used fit. observations included default. na.action function filter missing data. Hess logical whether Hessian (observed information matrix) returned. model logical whether model matrix returned.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/osm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ordinal data regression using the Ordered Stereotype Model (OSM). — osm","text":"object class \"osm\".  components beta coefficients covariates, intercept. mu intercepts categories. phi score parameters categories (restricted   ordered). deviance residual deviance. fitted.values matrix fitted values, column   level response. lev names response levels. terms terms structure describing model. df.residual number residual degrees freedom, calculated   using weights. edf (effective) number degrees freedom used model n, nobs (effective) number observations, calculated using   weights. call matched call. convergence convergence code returned optim. niter number function gradient evaluations used   optim. eta Hessian (Hess true).  Note numerical   approximation derived optimization proces. model (model true), model used fitting. na.action NA function used xlevels factor levels categorical predictors","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/osm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ordinal data regression using the Ordered Stereotype Model (OSM). — osm","text":"function used similar way MASS::polr, arguments polr, ordinal model used less restrictive assumptions proportional odds model. However, still parsimonious .e. uses small number additional parameters compared proportional odds model. model ordered stereotype model (Anderson 1984, Agresti 2010) flexible proportional odds model adds handful additional parameters. cumulative model, instead defined terms relationships higher categories lowest category treated reference category. higher categories intercept term, mu_k, similar zeta parameters polr, OSM higher category also scaling parameter, phi_k, adjusts effect covariates response. allows effect covariates response slightly different category response, thus making model flexible proportional odds model. final set parameters coefficients covariates, equivalent coefs polr. Higher positive values coefficients increases probability response higher categories, lower negative values coefficients increase probability response lower categories. overall model takes following form: log(P(Y = k | X)/P(Y = 1 | X)) = mu_k + phi_k*beta_vec^T x_vec k = 2, ..., q, x_vec vector covariates observation Y. mu_1 fixed 0 identifiability model, phi_k parameters constrained ordered (giving model name) following way: 0 = phi_1 <= phi_2 <= ... <= phi_k <= ... <= phi_q = 1. (unordered stereotype model restricts phi_1 phi_q allows remaining phi_k order, suitable fitting model nominal data. However, package provide option, already available packages can fit stereotype model.) fitting model, estimated values intermediate phi_k values indicate suitable numerical spacing ordinal response categories based data. spacings indicate much distinct information corresponding levels provide. example, five response categories fitted phi values (0, 0.04, 0.6, 0.62, 1) indicates levels 1 2 provide similar information effect covariates response, levels 3 4 provide similar information . meaning simplify response combining levels 1 2 combining levels 3 4 (.e. reduce levels 1, 3 5) still able estimate beta coefficients similar accuracy. Another use phi_k values want carry analysis response, treating numerical variable, phi values better choice numerical values response categories default values 1 q. start argument values: start vector start values estimating model parameters. first part start vector starting values coefficients covariates, second part starting values mu values (per-category intercepts), third part starting values raw parameters used construct phi values. length vector [number covariate terms] + [number categories response variable - 1] + [number categories response variable - 2]. Every one values can take real value. second part starting values mu_k per-category intercept parameters, since mu_1 fixed 0 identifiability, number non-fixed mu_k parameters one fewer number categories. third part starting vector re-parametrization used construct starting values estimated phi parameters phi parameters observe ordering restriction ordered stereotype model, raw parameters restricted makes easier optimise . phi_1 always 0 phi_q always 1 (q number response categories). raw parameters u_2 u_(q-1), phi_2 constructed expit(u_2), phi_3 expit(u_2 + exp(u_3)), phi_4 expit(exp(u_3) + exp(u_4)) etc. ensures phi_k values non-decreasing. code adapted file MASS/R/polr.R copyright (C) 1994-2013 W. N. Venables B. D. Ripley Use transformed intercepts contributed David Firth osm osm.fit functions written Louise McMillan, 2020. program free software; can redistribute /modify terms GNU General Public License published Free Software Foundation; either version 2 3 License (option). program distributed hope useful, WITHOUT WARRANTY; without even implied warranty MERCHANTABILITY FITNESS PARTICULAR PURPOSE.  See GNU General Public License details. copy GNU General Public License available http://www.r-project.org/Licenses/","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/osm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Ordinal data regression using the Ordered Stereotype Model (OSM). — osm","text":"Agresti, . (2010). Analysis ordinal categorical data (Vol. 656). John Wiley & Sons. Anderson, J. . (1984). Regression ordered categorical variables. Journal Royal Statistical Society: Series B (Methodological), 46(1), 1-22.","code":""},{"path":[]},{"path":"https://vuw-clustering.github.io/clustord/reference/rerun.html","id":null,"dir":"Reference","previous_headings":"","what":"Rerun clustord using the results of a previous run as the starting point. — rerun","title":"Rerun clustord using the results of a previous run as the starting point. — rerun","text":"function designed two purposes. (1) tried run clustord results converge. can supply function previous results previous data object, carry running clustord endpoint previous run, quicker starting run scratch iterations.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/rerun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rerun clustord using the results of a previous run as the starting point. — rerun","text":"","code":"rerun(   results.original,   long.df,   EM.control = NULL,   verbose = FALSE,   optim.control = NULL )"},{"path":"https://vuw-clustering.github.io/clustord/reference/rerun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rerun clustord using the results of a previous run as the starting point. — rerun","text":"results.original results previous run want use starting point. model, number clusters, final parameter values used, cluster controls EMcycles reused unless user specifies new values. row cluster /column cluster memberships reused, dataset, can change dataset slightly rest details applied new dataset. long.df dataset use run, may slightly different original. Please note compatibility check performed comparing sizes original new datasets, user check new dataset sufficiently similar old one. EM.control Options use run EMcycles (number EM iterations). Note \"startEMcycles\" relevant run generate random starts, run end parameters run. See clustord documentation info. verbose (default FALSE) changes much reported console algorithm's progress. See clustord documentation info. optim.control Options use run within optim(), used estimate parameters M-step. See clustord documentation info.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/rerun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rerun clustord using the results of a previous run as the starting point. — rerun","text":"object class clustord. See clustord info.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/rerun.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rerun clustord using the results of a previous run as the starting point. — rerun","text":"(2) previous result converged, changed dataset slightly, want rerun previous endpoint save time. Either way, call function way, supplying previous results object dataset, optionally new number iterations (`EM.control=list(EMcycles=XXX)`, `XXX` new number iterations.) output parameters old result used new initial parameters.","code":""},{"path":"https://vuw-clustering.github.io/clustord/reference/rerun.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rerun clustord using the results of a previous run as the starting point. — rerun","text":"","code":"if (FALSE) { # \\dontrun{ results.original <- clustord(Y ~ ROWCLUST, model=\"OSM\", nclus.row=4,                              long.df=long.df, EM.control=list(EMcycles=50)) results.original$EM.status$converged # FALSE  ## Since original run did not converge, rerun from that finishing point and ## allow more iterations this time results.new <- rerun(results.original, long.df, EM.control=list(EMcycles=200))  ## Alternatively, if dataset has changed slightly then rerun from the ## previous finishing point to give the new results a helping hand long.df.new <- long.df[-c(4,25,140),] results.new <- rerun(results.original, long.df.new) } # }"}]
